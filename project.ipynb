{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context – Drunk Smurfs\n",
    "Among all international hotel guests, Smurfs are burdened with the upkeep of a singular reputation: they are (supposedly) the rowdiest bunch one can entertain, and are equally well-known for unbridled spending as for racking up extensive costs in damages to hotel infrastructure, staff, and occasionally also other guests – costs which typically cannot be recovered once the guest has sought out the safety of his (or her) homeland.\n",
    "It is your job as a data scientist to screen applying Smurfs clients for an exclusive hotel in the Bahamas - yes, it's the kind of hotel you need to apply for!\n",
    "# The data\n",
    "At your disposal is a training set containing data about the behavior of 5000 Smurf hotel guests (train_V2.csv). This data set contains information about the profit the hotel made during their last visit (excluding damages), but also whether they caused damages during their last visit, and for what amount. These outcomes are respectively called 'outcome_profit', 'outcome_damage_inc', and 'outcome_damage_amount'. To predict them, you have access to a host of personal information: previous history of profits and damages, use of hotel facilities, socio-demographics and behavioral scores from the staff of other hotels within the hotel chains. A minor description of features is available in dictionary.csv.\n",
    "You also get information on the 500 applicants for the 2024 season (score.csv). It is your job to return a list of 150 clients that offer an attractive balance between projected profit for the hotel, and anticipated damages. \n",
    "You will notice the data set contains a large number of oddities. You are expected to think yourself about what is intuitive and acceptable in terms of approach, and to provide some minor reflection on this in your technical report. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible approach\n",
    "To generate a client list, you can (but don't have to) follow the next steps:\n",
    "1)\tprepare the data set\t\n",
    "* briefly survey the data\n",
    "* deal with data issues:\n",
    "* appropriate handle categorical data\n",
    "* treat missing data\n",
    "* identify outliers, and choose whether to make your analysis more robust by removing these\n",
    "2)\tpredict the projected revenue per clients\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "3)\tpredict which clients will cause damage\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "4)\tfor those that will wreak havoc, predict the amount of damage they will cause\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "5)\tcreate a measure of the expected value of each applicant, and create an optimal selection of 200 guests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('train_V2.csv')\n",
    "score = pd.read_csv('score.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. and 2: number of features and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr_matrix = train.corr()\n",
    "print(corr_matrix)\n",
    "# Geen variabelen die een correlatie van 1 hebben dus op basis daarvan moeten we geen variabelen weglaten.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no constant variables so we do not need to ommit any based on this information.\n",
    "constant_columns = [col for col in train.columns if train[col].nunique() == 1]\n",
    "print(constant_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No constant variables were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0:500].T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check for datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. and 5. Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we check how many missing values we have per variable.\n",
    "train.isnull().sum()[train.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we look at what percentage of the observations are not NaN per variable\n",
    "(5000- train.isnull().sum()[train.isnull().sum() != 0])/5000*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that cannot contain negative values\n",
    "non_neg_cols = ['outcome_damage_inc','outcome_damage_amount','crd_lim_rec', 'credit_use_ic', 'insurance_ic', 'spa_ic', \n",
    "                'empl_ic', 'bar_no', 'sport_ic','neighbor_income','age', 'dining_ic', \n",
    "                'presidential', 'client_segment', 'sect_empl','prev_stay','prev_all_in_stay', 'fam_adult_size', 'children_no','tenure_yrs',\n",
    "                'tenure_mts','company_ic', 'claims_no','claims_am', 'damage_am', 'damage_inc','nights_booked', 'shop_am', 'shop_use', 'retired',\n",
    "                'profit_am','profit_last_am', 'gold_status']\n",
    "\n",
    "mask = (train[non_neg_cols] < 0).any(axis=1)\n",
    "\n",
    "# Drop the rows that have negative values in any of the specified columns\n",
    "train = train[~mask]\n",
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we look at the first 16 variables\n",
    "train.iloc[:,0:16].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we look at the variables starting from the 16th just to see what the data looks like\n",
    "train.iloc[:,16:53].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the amount of men and women with a bar chart\n",
    "sns.countplot(y=train[\"gender\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Add title to the Heat map\n",
    "title = \"Correlation between variables heatmap\"\n",
    "\n",
    "# Set the font size and the distance of the title from the plot\n",
    "plt.title(title,fontsize=18)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5,1.05])\n",
    "\n",
    "# Hide ticks for X & Y axis\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Remove the axes\n",
    "ax.axis('off')\n",
    "\n",
    "sns.heatmap(corrmat,fmt=\"\",cmap='RdYlGn',linewidths=0.30,ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_damage_inc\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_profit\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_damage_amount\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the z_scores of each data point and identify outliers as data points with a score greater than 3, here for 'outcome_damage_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_damage_amount'] - train['outcome_damage_amount'].mean()) / train['outcome_damage_amount'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_damage_amount\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the z_scores of each data point and identify outliers as data points with a score greater than 3, here for 'outcome_profit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_profit'] - train['outcome_profit'].mean()) / train['outcome_profit'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_profit\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the z_scores of each data point and identify outliers as data points with a score greater than 3, here for 'outcome_damage_inc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_damage_inc'] - train['outcome_damage_inc'].mean()) / train['outcome_damage_inc'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_damage_inc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Look at the descriptives\n",
    "1. For which features do you suspect outliers?\n",
    "2. Which of these outliers seem most suspicious? Which would you certainly check if you were able to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[:,0:16].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze variabele is een boolean maar moet een getal worden\n",
    "train['married_cd'] = train['married_cd'].astype('int')\n",
    "train.loc[:, 'married_cd']\n",
    "\n",
    "score['married_cd'] = score['married_cd'].astype('int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates()\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replace all NaN values with '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN values with a specified value (e.g., 0)\n",
    "train.fillna(-1, inplace=True)\n",
    "train.head()\n",
    "\n",
    "score.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = train.isna()\n",
    "columns_with_na = train.columns[na.any()].tolist() \n",
    "print(len(columns_with_na))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop irrelvant columns\n",
    "\n",
    "gluten_ic and lactose_ic: The fact that a person is gluten or lactose intolerant does not indicate how likely it is for them to inflict damages or how much money they will be spending in the hotel. Maybe they will pay a tiny bit more for food without those ingredients but it shouldn't have a significant impact.\n",
    "\n",
    "cab_requests: The hotel is very unlikely to own the cab company so wether or not they buy a lot of taxis will not influence the profit for the hotel.\n",
    "\n",
    "marketing_permit: The choice on wether or not the marketing team may contact them will not influence how much money they will be spending nor how likely they are to inflict damages.\n",
    "\n",
    "region: Although region could be a small factor due to cultural diferences in spending and personal traits, this could lead to discrimination of people of a certain region.\n",
    "\n",
    "gender: Here it could also be that a cerain gender is for example more aggressive than others and thus more likely to inflict damages, but this could also lead to discrimination based on generalisations.\n",
    "\n",
    "divorce: Being divorced or not does not impact the way you behave or spend money, definitely not if some time has passed. Maybe the first months they could be a bit more aggressive or impulsive due to their grief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "train = train.drop(['gluten_ic', 'lactose_ic', 'marketing_permit','divorce', 'cab_requests', 'urban_ic', 'gender', \"married_cd\"], axis=1) #outcome_damage_inc, outcome_damage_amount\n",
    "score = score.drop(['gluten_ic', 'lactose_ic', 'marketing_permit', 'divorce', 'cab_requests', 'urban_ic', 'gender', \"married_cd\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I make sure all three categorical features are classified as 'object' to be able to check if they are categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove unwanted outliers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No unwanted outliers found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the different ML models to predict projected revenue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.0 Split the data in test/train and standardize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = train\n",
    "X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "y = train['outcome_profit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_feat])\n",
    "\n",
    "X_train_stan = X_train.copy()\n",
    "X_test_stan = X_test.copy()\n",
    "\n",
    "X_train_stan[num_feat] = scaler.transform(X_train[num_feat])\n",
    "X_test_stan[num_feat] = scaler.transform(X_test[num_feat])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Lineair Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit algoritme heeft een score van 0.2842767456180185\n",
    "\n",
    "# Train a linear regression model\n",
    "LRmodel = LinearRegression()\n",
    "LRmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "testScore = LRmodel.score(X_test, y_test)\n",
    "print('R^2 score on testing set:', testScore)\n",
    "\n",
    "LRy_pred = LRmodel.predict(X_test)\n",
    "\n",
    "# Predict the projected revenue for the 500 applicants\n",
    "\n",
    "LRscore = scaler.transform(score)\n",
    "LRpredictions = LRmodel.predict(LRscore)\n",
    "\n",
    "# Sort the predictions in descending order\n",
    "sorted_index = np.argsort(LRpredictions)[::-1]\n",
    "LRsorted_predictions = LRpredictions[sorted_index]\n",
    "\n",
    "print(LRsorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Actual outcomes and predicted outcomes: Lineair Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, LRy_pred)\n",
    "plt.xlabel('Actual outcomes')\n",
    "plt.ylabel('Predicted outcomes')\n",
    "plt.title('Scatter plot of actual vs predicted outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heeft een score van 0.025000182087153267\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Train a linear regression model\n",
    "KNNmodel = KNeighborsRegressor(n_neighbors=5)\n",
    "KNNmodel.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "KNNy_pred = KNNmodel.predict(X_test)\n",
    "\n",
    "# Evaluate the model using r-squared score\n",
    "r2 = r2_score(y_test, KNNy_pred)\n",
    "print('R-squared score:', r2)\n",
    "\n",
    "# Predict the projected revenue for the 500 applicants\n",
    "KNNscore = scaler.transform(score)\n",
    "KNNpredictions = KNNmodel.predict(KNNscore)\n",
    "\n",
    "# Sort the predictions in descending order\n",
    "KNNsorted_index = np.argsort(KNNpredictions)[::-1]\n",
    "KNNsorted_predictions = KNNpredictions[KNNsorted_index]\n",
    "\n",
    "print(KNNsorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Actual outcomes and predicted outcomes: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted outcomes against the actual outcomes in the testing set\n",
    "plt.scatter(y_test, KNNy_pred)\n",
    "plt.xlabel('Actual outcomes')\n",
    "plt.ylabel('Predicted outcomes')\n",
    "plt.title('Scatter plot of actual vs predicted outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit algoritme heeft een score van 0.5646249872330892\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regressor object\n",
    "dt_regressor = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# Fit the regressor with the training data\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict the revenue on the testing data\n",
    "y_pred_dt = dt_regressor.predict(X_test)\n",
    "\n",
    "# Compute R^2 score on the testing data\n",
    "r2_score_dt = dt_regressor.score(X_test, y_test)\n",
    "print(\"R^2 Score (Decision Tree Regression): \", r2_score_dt)\n",
    "\n",
    "# Predict the projected revenue for the 500 applicants\n",
    "\n",
    "DTscore = scaler.transform(score)\n",
    "DTpredictions = dt_regressor.predict(DTscore)\n",
    "\n",
    "# Sort the predictions in descending order\n",
    "DTsorted_index = np.argsort(DTpredictions)[::-1]\n",
    "DTsorted_predictions = DTpredictions[DTsorted_index]\n",
    "\n",
    "print(DTsorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Actual outcomes and predicted outcomes: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_dt)\n",
    "plt.xlabel('Actual outcomes')\n",
    "plt.ylabel('Predicted outcomes')\n",
    "plt.title('Scatter plot of actual vs predicted outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.4 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit algoritme heeft een score van 0.7581997911409797\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate the model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "RFy_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "RFr2 = r2_score(y_test, RFy_pred)\n",
    "print(\"r2 score on test set:\", RFr2)\n",
    "\n",
    "# Predict the projected revenue for the 500 applicants\n",
    "\n",
    "RFscore = scaler.transform(score)\n",
    "RFpredictions = rf.predict(RFscore)\n",
    "\n",
    "# Sort the predictions in descending order\n",
    "RFsorted_index = np.argsort(RFpredictions)[::-1]\n",
    "RFsorted_predictions = RFpredictions[RFsorted_index]\n",
    "\n",
    "print(RFsorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Actual outcomes and predicted outcomes: Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, RFy_pred)\n",
    "plt.xlabel('Actual outcomes')\n",
    "plt.ylabel('Predicted outcomes')\n",
    "plt.title('Scatter plot of actual vs predicted outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.5 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit algoritme heeft een score van 0.771681378659433\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Instantiate the model\n",
    "GBmodel = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model on the training data\n",
    "GBmodel.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "GBy_pred = GBmodel.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared score on the testing data\n",
    "GBr2 = r2_score(y_test, GBy_pred)\n",
    "print(\"R-squared score on testing data:\", GBr2)\n",
    "\n",
    "# Predict the projected revenue for the 500 applicants\n",
    "\n",
    "GBscore = scaler.transform(score)\n",
    "GBpredictions = GBmodel.predict(GBscore)\n",
    "\n",
    "# Sort the predictions in descending order\n",
    "GBsorted_index = np.argsort(GBpredictions)[::-1]\n",
    "GBsorted_predictions = GBpredictions[GBsorted_index]\n",
    "\n",
    "print(GBsorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Actual outcomes and predicted outcomes: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, GBy_pred)\n",
    "plt.xlabel('Actual outcomes')\n",
    "plt.ylabel('Predicted outcomes')\n",
    "plt.title('Scatter plot of actual vs predicted outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.6.1 Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = train\n",
    "X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "y = train['outcome_profit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of features\n",
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the linear regression\n",
    "reg_quad = LinearRegression(fit_intercept=False)\n",
    "reg_quad.fit(X_train_poly, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_quad.score(X_train_poly, y_train))\n",
    "print(reg_quad.score(X_test_poly, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is overfitted since it performs relatively well on the training set but not on the new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the evaluation metric (e.g., mean squared error)\n",
    "metric = 'neg_mean_squared_error'\n",
    "\n",
    "# Evaluate each algorithm using 10-fold cross-validation\n",
    "scores = {}\n",
    "for reg in [KNNmodel, reg_quad, GBmodel, rf, dt_regressor, LRmodel]:\n",
    "    name = type(reg).__name__\n",
    "    CVscore = cross_val_score(reg, X, y, cv=10, scoring=metric)\n",
    "    scores[name] = -CVscore.mean()\n",
    "\n",
    "# Print the mean squared error of each algorithm\n",
    "for name, CVscore in scores.items():\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion Rating ML Algorithms\n",
    "\n",
    "KNeighborsRegressor: 1543567.6565\n",
    "LinearRegression: 1290593.4049\n",
    "GradientBoostingRegressor: 422058.4695\n",
    "RandomForestRegressor: 486162.5380\n",
    "DecisionTreeRegressor: 904579.7700\n",
    "\n",
    "We can see that gradientboosting has the lowest mean squared error so for this metric the gradientboosting scores the best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Damages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the 500 applicants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in test/train and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_feat])\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "X_test_stand[num_feat] = scaler.transform(X_test[num_feat])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fit decision tree regressor with cross-validation\n",
    "depth = np.arange(1, 50)\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "for d in depth:\n",
    "    dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "    scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# fit decision tree regressor to entire training set\n",
    "dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# Standardize numerical features for new applicants\n",
    "num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "new_applicants_stand = score.copy()\n",
    "new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "\n",
    "# Predict damages for new applicants using the trained decision tree regressor\n",
    "damages_pred = dec_tree.predict(new_applicants_stand)\n",
    "\n",
    "# Only keep the applicants who will cause damage to calculate the damage amount\n",
    "applicants_who_will_cause_damage = new_applicants_stand[damages_pred > 0]\n",
    "applicants_who_will_cause_damage\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fit decision tree regressor with cross-validation\n",
    "depth = np.arange(1, 50)\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "for d in depth:\n",
    "    dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "    scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# fit decision tree regressor to entire training set\n",
    "dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# Standardize numerical features for new applicants\n",
    "num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "new_applicants_stand = score.copy()\n",
    "new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# Predict damages for new applicants using the trained decision tree regressor\n",
    "damages_pred = dec_tree.predict(new_applicants_stand)\n",
    "print(damages_pred)\n",
    "\n",
    "# Only keep the applicants who will cause damage to calculate the damage amount\n",
    "applicants_who_will_cause_damage = new_applicants_stand[damages_pred > 0]\n",
    "applicants_who_will_cause_damage\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fit KNN regressor with cross-validation\n",
    "k_values = np.arange(1, 50)\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_stand, y_train, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# fit KNN regressor to entire training set\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_train_stand, y_train)\n",
    "\n",
    "# Standardize numerical features for new applicants\n",
    "num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "new_applicants_stand = score.copy()\n",
    "new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# Predict damages for new applicants using the trained KNN regressor\n",
    "damages_pred = knn.predict(new_applicants_stand)\n",
    "print(damages_pred)\n",
    "\n",
    "# Only keep the applicants who will cause damage to calculate the damage amount\n",
    "applicants_who_will_cause_damage = new_applicants_stand[damages_pred > 0]\n",
    "applicants_who_will_cause_damage\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.5 Lineair Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_stand, y_train)\n",
    "\n",
    "# Standardize numerical features for new applicants\n",
    "new_applicants_stand = score.copy()\n",
    "new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# Predict damages for new applicants using the trained linear regression model\n",
    "damages_pred = lin_reg.predict(new_applicants_stand)\n",
    "print(damages_pred)\n",
    "\n",
    "# Only keep the applicants who will cause damage to calculate the damage amount\n",
    "applicants_who_will_cause_damage = new_applicants_stand[damages_pred > 0]\n",
    "applicants_who_will_cause_damage\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fit random forest regressor with cross-validation\n",
    "depth = np.arange(1, 50)\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "for d in depth:\n",
    "    rnd_forest = RandomForestRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "    scores = cross_val_score(rnd_forest, X_train_stand, y_train, cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# fit random forest regressor to entire training set\n",
    "rnd_forest.fit(X_train_stand, y_train)\n",
    "\n",
    "# Standardize numerical features for new applicants\n",
    "num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "new_applicants_stand = score.copy()\n",
    "new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# Predict damages for new applicants using the trained random forest regressor\n",
    "damages_pred = rnd_forest.predict(new_applicants_stand)\n",
    "print(damages_pred) \n",
    "\n",
    "# Only keep the applicants who will cause damage to calculate the damage amount\n",
    "applicants_who_will_cause_damage = new_applicants_stand[damages_pred > 0]\n",
    "applicants_who_will_cause_damage\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Predict damage amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO\n",
    " # Use all ML algorithms to predict damages:\n",
    "    # - Linear Regression x\n",
    "    # - Decision Tree x\n",
    "    # - KNN x\n",
    "    # - Random Forest x\n",
    "    # - Gradient Boosting x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit Gradient Boosting regressor with cross-validation\n",
    "# depth = np.arange(1, 10)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     gb_regressor = GradientBoostingRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "#     scores = cross_val_score(gb_regressor, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit Gradient Boosting regressor to entire training set\n",
    "# gb_regressor.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Predict damages for new applicants using the trained Gradient Boosting regressor\n",
    "# damages_pred = gb_regressor.predict(applicants_who_will_cause_damage)\n",
    "# damages_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Decision Tree Regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Zorg ervoor dat de andere algoritmes ook de applicants_who_will_cause_damage gebruiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree Regressor\n",
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit decision tree regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "#     scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit decision tree regressor to entire training set\n",
    "# dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained decision tree regressor\n",
    "# damages_pred = dec_tree.predict(applicants_who_will_cause_damage)\n",
    "# print(damages_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit KNN regressor with cross-validation\n",
    "# k_values = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for k in k_values:\n",
    "#     knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#     scores = cross_val_score(knn, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit KNN regressor to entire training set\n",
    "# knn = KNeighborsRegressor(n_neighbors=5)\n",
    "# knn.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained KNN regressor\n",
    "# damages_pred = knn.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit random forest regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     rnd_forest = RandomForestRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "#     scores = cross_val_score(rnd_forest, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit random forest regressor to entire training set\n",
    "# rnd_forest.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained random forest regressor\n",
    "# damages_pred = rnd_forest.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# To select the 200 applicants we will subtract the predicted damages from the predicted revenue\n",
    "# Test the ML algortihms to determine which one is the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
