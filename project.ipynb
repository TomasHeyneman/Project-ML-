{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context – Drunk Smurfs\n",
    "Among all international hotel guests, Smurfs are burdened with the upkeep of a singular reputation: they are (supposedly) the rowdiest bunch one can entertain, and are equally well-known for unbridled spending as for racking up extensive costs in damages to hotel infrastructure, staff, and occasionally also other guests – costs which typically cannot be recovered once the guest has sought out the safety of his (or her) homeland.\n",
    "It is your job as a data scientist to screen applying Smurfs clients for an exclusive hotel in the Bahamas - yes, it's the kind of hotel you need to apply for!\n",
    "# The data\n",
    "At your disposal is a training set containing data about the behavior of 5000 Smurf hotel guests (train_V2.csv). This data set contains information about the profit the hotel made during their last visit (excluding damages), but also whether they caused damages during their last visit, and for what amount. These outcomes are respectively called 'outcome_profit', 'outcome_damage_inc', and 'outcome_damage_amount'. To predict them, you have access to a host of personal information: previous history of profits and damages, use of hotel facilities, socio-demographics and behavioral scores from the staff of other hotels within the hotel chains. A minor description of features is available in dictionary.csv.\n",
    "You also get information on the 500 applicants for the 2024 season (score.csv). It is your job to return a list of 150 clients that offer an attractive balance between projected profit for the hotel, and anticipated damages. \n",
    "You will notice the data set contains a large number of oddities. You are expected to think yourself about what is intuitive and acceptable in terms of approach, and to provide some minor reflection on this in your technical report. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible approach\n",
    "To generate a client list, you can (but don't have to) follow the next steps:\n",
    "1)\tprepare the data set\t\n",
    "* briefly survey the data\n",
    "* deal with data issues:\n",
    "* appropriate handle categorical data\n",
    "* treat missing data\n",
    "* identify outliers, and choose whether to make your analysis more robust by removing these\n",
    "2)\tpredict the projected revenue per clients\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "3)\tpredict which clients will cause damage\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "4)\tfor those that will wreak havoc, predict the amount of damage they will cause\n",
    "* choose an algorithm, and train it in an optimal way\n",
    "* score the 500 applicants\n",
    "5)\tcreate a measure of the expected value of each applicant, and create an optimal selection of 200 guests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joery\\AppData\\Local\\Temp\\ipykernel_8704\\1617307288.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('train_V2.csv')\n",
    "score = pd.read_csv('score.csv')\n",
    "#dictionary = pd.read_csv('dictionary.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. and 2: number of features and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 53)"
      ]
     },
     "execution_count": 1214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       income_am  profit_last_am  profit_am  damage_am  \\\n",
      "income_am               1.000000        0.716032   0.685038   0.102339   \n",
      "profit_last_am          0.716032        1.000000   0.424888   0.099840   \n",
      "profit_am               0.685038        0.424888   1.000000   0.164160   \n",
      "damage_am               0.102339        0.099840   0.164160   1.000000   \n",
      "damage_inc              0.022713        0.046216   0.146981   0.622453   \n",
      "crd_lim_rec             0.112643        0.058381   0.208033   0.288919   \n",
      "credit_use_ic           0.043790        0.068244   0.114818   0.101889   \n",
      "gluten_ic               0.158987        0.115330   0.167896   0.037240   \n",
      "lactose_ic              0.287967        0.139112   0.404624   0.017174   \n",
      "insurance_ic            0.006980        0.017561   0.058299   0.173917   \n",
      "spa_ic                  0.000436        0.004509   0.038036   0.021302   \n",
      "empl_ic                -0.018407       -0.008958  -0.016365   0.015352   \n",
      "cab_requests            0.055035        0.031911   0.083532   0.067957   \n",
      "married_cd             -0.003605       -0.002442  -0.000405   0.035559   \n",
      "bar_no                  0.050755        0.022140   0.087099   0.083122   \n",
      "sport_ic                0.063597        0.064670   0.196985   0.278828   \n",
      "neighbor_income         0.074094        0.048356   0.062706   0.055634   \n",
      "age                     0.125520        0.059197   0.121624  -0.120778   \n",
      "marketing_permit        0.109794        0.070896   0.209002   0.087981   \n",
      "urban_ic               -0.004030       -0.009857   0.021784  -0.007513   \n",
      "dining_ic               0.032636        0.018224   0.081174   0.033081   \n",
      "presidential            0.014427        0.020852   0.029272   0.009360   \n",
      "client_segment          0.464069        0.276922   0.514530   0.106580   \n",
      "sect_empl               0.041485        0.032261   0.103744   0.131699   \n",
      "prev_stay               0.052175        0.042366   0.057999   0.013470   \n",
      "prev_all_in_stay        0.160085        0.098709   0.247231   0.173880   \n",
      "divorce                -0.002003       -0.003032  -0.001525  -0.025047   \n",
      "fam_adult_size         -0.013533        0.021152   0.024745   0.235889   \n",
      "children_no            -0.028462        0.002107   0.023758   0.181949   \n",
      "tenure_mts              0.112839        0.061204   0.154008   0.001874   \n",
      "tenure_yrs              0.112506        0.060954   0.154169   0.002309   \n",
      "company_ic              0.004311        0.006406   0.011365   0.046748   \n",
      "claims_no               0.048024        0.022067   0.106383   0.057227   \n",
      "claims_am               0.018537       -0.004617   0.047086   0.001333   \n",
      "nights_booked           0.055027        0.043635   0.149183   0.179587   \n",
      "shop_am                 0.101521        0.053918   0.164232   0.047551   \n",
      "shop_use                0.057650        0.030831   0.129462   0.103116   \n",
      "retired                 0.078020        0.041426   0.096737  -0.103352   \n",
      "gold_status             0.191957        0.081880   0.303512  -0.014199   \n",
      "score1_pos              0.024567        0.054579  -0.039588  -0.083314   \n",
      "score1_neg             -0.021546        0.005491  -0.041182   0.035635   \n",
      "score2_pos             -0.037809       -0.045797  -0.042096  -0.134875   \n",
      "score2_neg              0.026308        0.014415   0.056596   0.109746   \n",
      "score3_pos             -0.031287       -0.006170  -0.026177  -0.090153   \n",
      "score3_neg             -0.018090        0.016287   0.006820   0.091163   \n",
      "score4_pos              0.038840        0.031997  -0.000291  -0.018802   \n",
      "score4_neg              0.019455        0.017195   0.031301   0.038679   \n",
      "score5_pos              0.117703        0.058429   0.079077  -0.105444   \n",
      "score5_neg             -0.003788        0.004711   0.010049  -0.000111   \n",
      "outcome_profit          0.447493        0.357298   0.452913   0.101715   \n",
      "outcome_damage_inc      0.032948        0.035833   0.030561   0.058629   \n",
      "outcome_damage_amount   0.068846        0.065361   0.073691   0.125705   \n",
      "\n",
      "                       damage_inc  crd_lim_rec  credit_use_ic  gluten_ic  \\\n",
      "income_am                0.022713     0.112643       0.043790   0.158987   \n",
      "profit_last_am           0.046216     0.058381       0.068244   0.115330   \n",
      "profit_am                0.146981     0.208033       0.114818   0.167896   \n",
      "damage_am                0.622453     0.288919       0.101889   0.037240   \n",
      "damage_inc               1.000000     0.399707       0.161317   0.046919   \n",
      "crd_lim_rec              0.399707     1.000000       0.171585   0.107887   \n",
      "credit_use_ic            0.161317     0.171585       1.000000   0.170227   \n",
      "gluten_ic                0.046919     0.107887       0.170227   1.000000   \n",
      "lactose_ic               0.076109     0.213431       0.173263   0.274450   \n",
      "insurance_ic             0.218264     0.272985       0.252607   0.110328   \n",
      "spa_ic                   0.065672     0.061756       0.092586   0.038110   \n",
      "empl_ic                  0.031441     0.039943       0.026393  -0.008303   \n",
      "cab_requests             0.096829     0.131315       0.063776   0.042759   \n",
      "married_cd               0.041171     0.068005      -0.016671   0.016792   \n",
      "bar_no                   0.122399     0.214109       0.125400   0.078212   \n",
      "sport_ic                 0.468609     0.349244       0.174035   0.077736   \n",
      "neighbor_income          0.068737     0.084643       0.020100   0.043794   \n",
      "age                     -0.118492    -0.008140      -0.044355   0.021295   \n",
      "marketing_permit         0.125664     0.226347       0.107619   0.050977   \n",
      "urban_ic                 0.055527     0.032566      -0.001047   0.020982   \n",
      "dining_ic                0.098205     0.086809       0.164778   0.048290   \n",
      "presidential             0.036897     0.042272       0.017639   0.029641   \n",
      "client_segment           0.091286     0.267597       0.078832   0.176728   \n",
      "sect_empl                0.115839     0.112917       0.025176   0.026761   \n",
      "prev_stay                0.070432     0.056030       0.046997   0.043462   \n",
      "prev_all_in_stay         0.299489     0.453846       0.199918   0.135492   \n",
      "divorce                 -0.022208    -0.041822       0.033831  -0.006461   \n",
      "fam_adult_size           0.356119     0.285773       0.105969   0.040065   \n",
      "children_no              0.305602     0.288330       0.111621   0.021941   \n",
      "tenure_mts               0.085730     0.205588       0.036015   0.088909   \n",
      "tenure_yrs               0.086107     0.205533       0.036321   0.089260   \n",
      "company_ic               0.034633     0.027141       0.016602   0.007054   \n",
      "claims_no                0.134803     0.123379       0.201892   0.095815   \n",
      "claims_am                0.011005     0.045138       0.016410   0.013515   \n",
      "nights_booked            0.367099     0.489951       0.232192   0.085308   \n",
      "shop_am                  0.138910     0.239882       0.104676   0.092599   \n",
      "shop_use                 0.203036     0.262488       0.156423   0.092840   \n",
      "retired                 -0.150439    -0.050213      -0.034650   0.019520   \n",
      "gold_status              0.007939     0.070007       0.077177   0.133461   \n",
      "score1_pos              -0.130509    -0.032748      -0.046194  -0.020465   \n",
      "score1_neg               0.091896     0.074485      -0.000833  -0.032166   \n",
      "score2_pos              -0.223048    -0.067402      -0.044653  -0.023848   \n",
      "score2_neg               0.110204     0.065940       0.029570   0.041516   \n",
      "score3_pos              -0.155767    -0.030523      -0.034306  -0.021414   \n",
      "score3_neg               0.131510     0.085251       0.046419  -0.040186   \n",
      "score4_pos              -0.066072    -0.005072      -0.026160  -0.015196   \n",
      "score4_neg               0.011735     0.016339       0.002972  -0.038815   \n",
      "score5_pos              -0.118458    -0.036733      -0.039996   0.034343   \n",
      "score5_neg               0.110729     0.013646      -0.014183   0.027131   \n",
      "outcome_profit           0.043003    -0.062730       0.015491   0.069464   \n",
      "outcome_damage_inc       0.056422     0.051984       0.055543   0.023313   \n",
      "outcome_damage_amount    0.128011     0.104761       0.092227   0.038610   \n",
      "\n",
      "                       lactose_ic  insurance_ic  ...  score2_neg  score3_pos  \\\n",
      "income_am                0.287967      0.006980  ...    0.026308   -0.031287   \n",
      "profit_last_am           0.139112      0.017561  ...    0.014415   -0.006170   \n",
      "profit_am                0.404624      0.058299  ...    0.056596   -0.026177   \n",
      "damage_am                0.017174      0.173917  ...    0.109746   -0.090153   \n",
      "damage_inc               0.076109      0.218264  ...    0.110204   -0.155767   \n",
      "crd_lim_rec              0.213431      0.272985  ...    0.065940   -0.030523   \n",
      "credit_use_ic            0.173263      0.252607  ...    0.029570   -0.034306   \n",
      "gluten_ic                0.274450      0.110328  ...    0.041516   -0.021414   \n",
      "lactose_ic               1.000000      0.101854  ...    0.047547    0.015987   \n",
      "insurance_ic             0.101854      1.000000  ...    0.033578   -0.059634   \n",
      "spa_ic                   0.048121      0.138123  ...    0.008964   -0.032285   \n",
      "empl_ic                  0.020617      0.042093  ...   -0.006717    0.009339   \n",
      "cab_requests             0.080820      0.077182  ...   -0.025164    0.006429   \n",
      "married_cd               0.005111      0.037683  ...   -0.102693   -0.042638   \n",
      "bar_no                   0.093536      0.195088  ...   -0.017141   -0.028068   \n",
      "sport_ic                 0.179343      0.173860  ...    0.102355   -0.037865   \n",
      "neighbor_income          0.063063      0.001928  ...   -0.028368   -0.000991   \n",
      "age                      0.108476     -0.412481  ...    0.041095    0.094172   \n",
      "marketing_permit         0.195323      0.193713  ...    0.048267   -0.041226   \n",
      "urban_ic                 0.026081     -0.043459  ...    0.015215    0.031007   \n",
      "dining_ic                0.070755      0.047589  ...   -0.031441   -0.045813   \n",
      "presidential             0.042675      0.023832  ...    0.027730   -0.014427   \n",
      "client_segment           0.399155      0.018922  ...    0.069761    0.002119   \n",
      "sect_empl                0.058255      0.054252  ...    0.050082   -0.030740   \n",
      "prev_stay                0.084736     -0.035638  ...         NaN         NaN   \n",
      "prev_all_in_stay         0.242462      0.266279  ...    0.027540   -0.015714   \n",
      "divorce                 -0.006295      0.022935  ...    0.060782    0.046929   \n",
      "fam_adult_size           0.034521      0.215215  ...    0.044091   -0.027116   \n",
      "children_no              0.026217      0.189635  ...    0.046963    0.006086   \n",
      "tenure_mts               0.181332     -0.156032  ...    0.014058    0.116673   \n",
      "tenure_yrs               0.181074     -0.155588  ...    0.014070    0.116147   \n",
      "company_ic               0.027322      0.006234  ...    0.010025   -0.036886   \n",
      "claims_no                0.095463      0.039418  ...    0.030632   -0.098641   \n",
      "claims_am                0.041245      0.018583  ...    0.052055    0.003248   \n",
      "nights_booked            0.168838      0.209131  ...    0.045809   -0.013559   \n",
      "shop_am                  0.193069      0.067955  ...    0.045999    0.043491   \n",
      "shop_use                 0.192080      0.162865  ...    0.044622    0.010701   \n",
      "retired                  0.097051     -0.231027  ...    0.010682    0.047394   \n",
      "gold_status              0.346771     -0.011855  ...    0.013203   -0.011893   \n",
      "score1_pos              -0.012244     -0.059887  ...   -0.019733    0.206041   \n",
      "score1_neg               0.009099     -0.011964  ...   -0.059463    0.191489   \n",
      "score2_pos               0.001127     -0.034818  ...    0.355317    0.191327   \n",
      "score2_neg               0.047547      0.033578  ...    1.000000    0.190141   \n",
      "score3_pos               0.015987     -0.059634  ...    0.190141    1.000000   \n",
      "score3_neg               0.003189      0.033023  ...   -0.037285    0.342002   \n",
      "score4_pos              -0.010967     -0.061441  ...    0.151161    0.207152   \n",
      "score4_neg               0.006597     -0.029493  ...   -0.002729   -0.009151   \n",
      "score5_pos               0.029689      0.000719  ...    0.272552    0.272550   \n",
      "score5_neg              -0.002014      0.018237  ...    0.032101   -0.030930   \n",
      "outcome_profit           0.177831     -0.066770  ...    0.049560   -0.142681   \n",
      "outcome_damage_inc       0.050505      0.065004  ...    0.042525    0.016359   \n",
      "outcome_damage_amount    0.083122      0.114252  ...    0.075848   -0.016141   \n",
      "\n",
      "                       score3_neg  score4_pos  score4_neg  score5_pos  \\\n",
      "income_am               -0.018090    0.038840    0.019455    0.117703   \n",
      "profit_last_am           0.016287    0.031997    0.017195    0.058429   \n",
      "profit_am                0.006820   -0.000291    0.031301    0.079077   \n",
      "damage_am                0.091163   -0.018802    0.038679   -0.105444   \n",
      "damage_inc               0.131510   -0.066072    0.011735   -0.118458   \n",
      "crd_lim_rec              0.085251   -0.005072    0.016339   -0.036733   \n",
      "credit_use_ic            0.046419   -0.026160    0.002972   -0.039996   \n",
      "gluten_ic               -0.040186   -0.015196   -0.038815    0.034343   \n",
      "lactose_ic               0.003189   -0.010967    0.006597    0.029689   \n",
      "insurance_ic             0.033023   -0.061441   -0.029493    0.000719   \n",
      "spa_ic                   0.018098    0.033140   -0.026178    0.000567   \n",
      "empl_ic                 -0.010850   -0.029779   -0.033427    0.025977   \n",
      "cab_requests             0.023270   -0.019336   -0.013991   -0.007557   \n",
      "married_cd               0.027038   -0.094364   -0.009427   -0.009428   \n",
      "bar_no                  -0.003119   -0.020876   -0.035900   -0.039114   \n",
      "sport_ic                 0.062749    0.017326    0.029391   -0.032365   \n",
      "neighbor_income          0.016984   -0.046694   -0.032067   -0.000645   \n",
      "age                     -0.008068    0.086196    0.023559    0.067787   \n",
      "marketing_permit         0.041087    0.003970    0.041096    0.022875   \n",
      "urban_ic                -0.020090   -0.079686   -0.041634    0.030887   \n",
      "dining_ic                0.040840   -0.006462    0.043332    0.037956   \n",
      "presidential             0.040074    0.020667   -0.011408    0.028376   \n",
      "client_segment           0.003138    0.045212    0.028429    0.090709   \n",
      "sect_empl                0.009413    0.016879    0.037958   -0.058174   \n",
      "prev_stay                     NaN         NaN         NaN         NaN   \n",
      "prev_all_in_stay         0.092608    0.010093    0.006776    0.013826   \n",
      "divorce                 -0.060892    0.071175    0.020261    0.042388   \n",
      "fam_adult_size           0.077034   -0.033442    0.013821   -0.047932   \n",
      "children_no              0.053198    0.002101    0.011231   -0.005209   \n",
      "tenure_mts               0.015913    0.183820   -0.007162    0.063897   \n",
      "tenure_yrs               0.015469    0.182487   -0.007057    0.064318   \n",
      "company_ic              -0.018043    0.016898    0.030400   -0.013868   \n",
      "claims_no                0.086493   -0.007054    0.083562   -0.039019   \n",
      "claims_am                0.039537   -0.032598   -0.019297   -0.011655   \n",
      "nights_booked            0.085170    0.021740    0.011745   -0.018942   \n",
      "shop_am                  0.042785    0.004493    0.041901    0.025894   \n",
      "shop_use                 0.037744   -0.011681    0.010385   -0.006010   \n",
      "retired                 -0.017974    0.052956    0.037159    0.086102   \n",
      "gold_status             -0.020944    0.001565   -0.006724    0.011512   \n",
      "score1_pos              -0.027666    0.290050    0.086329    0.140795   \n",
      "score1_neg               0.059550    0.226101    0.040282    0.187432   \n",
      "score2_pos               0.001435    0.188313   -0.012863    0.245916   \n",
      "score2_neg              -0.037285    0.151161   -0.002729    0.272552   \n",
      "score3_pos               0.342002    0.207152   -0.009151    0.272550   \n",
      "score3_neg               1.000000    0.199850    0.008800    0.205935   \n",
      "score4_pos               0.199850    1.000000    0.328875    0.227991   \n",
      "score4_neg               0.008800    0.328875    1.000000    0.141449   \n",
      "score5_pos               0.205935    0.227991    0.141449    1.000000   \n",
      "score5_neg               0.132938    0.022998    0.001900    0.366369   \n",
      "outcome_profit          -0.040057   -0.062394   -0.047304    0.062191   \n",
      "outcome_damage_inc       0.038332    0.051443   -0.001732   -0.018134   \n",
      "outcome_damage_amount    0.014149    0.038020   -0.011870   -0.002802   \n",
      "\n",
      "                       score5_neg  outcome_profit  outcome_damage_inc  \\\n",
      "income_am               -0.003788        0.447493            0.032948   \n",
      "profit_last_am           0.004711        0.357298            0.035833   \n",
      "profit_am                0.010049        0.452913            0.030561   \n",
      "damage_am               -0.000111        0.101715            0.058629   \n",
      "damage_inc               0.110729        0.043003            0.056422   \n",
      "crd_lim_rec              0.013646       -0.062730            0.051984   \n",
      "credit_use_ic           -0.014183        0.015491            0.055543   \n",
      "gluten_ic                0.027131        0.069464            0.023313   \n",
      "lactose_ic              -0.002014        0.177831            0.050505   \n",
      "insurance_ic             0.018237       -0.066770            0.065004   \n",
      "spa_ic                   0.025786        0.020300            0.004620   \n",
      "empl_ic                  0.005488        0.001660           -0.008684   \n",
      "cab_requests             0.036953       -0.027889            0.164956   \n",
      "married_cd               0.016969       -0.020643            0.036159   \n",
      "bar_no                   0.023846        0.006936            0.232181   \n",
      "sport_ic                 0.017662       -0.004275            0.026519   \n",
      "neighbor_income          0.017320        0.115625            0.005907   \n",
      "age                      0.001791        0.045066           -0.022705   \n",
      "marketing_permit         0.032185        0.019832            0.045645   \n",
      "urban_ic                -0.006643        0.056666           -0.006109   \n",
      "dining_ic                0.027147        0.006855            0.001989   \n",
      "presidential             0.050582        0.177364            0.011526   \n",
      "client_segment           0.021363        0.231605            0.044160   \n",
      "sect_empl               -0.007232        0.031903            0.030278   \n",
      "prev_stay                0.019704        0.060551           -0.006863   \n",
      "prev_all_in_stay         0.024795       -0.012357            0.129407   \n",
      "divorce                  0.023671        0.018383           -0.025430   \n",
      "fam_adult_size           0.048840        0.000395            0.043347   \n",
      "children_no             -0.006029       -0.002237            0.001041   \n",
      "tenure_mts              -0.011863        0.010822            0.004041   \n",
      "tenure_yrs              -0.011859        0.010733            0.003861   \n",
      "company_ic               0.000276        0.016081            0.018777   \n",
      "claims_no                0.039542        0.015693            0.003143   \n",
      "claims_am                0.004160       -0.009411            0.005671   \n",
      "nights_booked            0.042181       -0.219676            0.017711   \n",
      "shop_am                 -0.031228        0.084892            0.030331   \n",
      "shop_use                -0.027901        0.042307            0.032005   \n",
      "retired                 -0.001376        0.027690           -0.036491   \n",
      "gold_status             -0.002778        0.159437            0.025338   \n",
      "score1_pos              -0.118530       -0.115641            0.003973   \n",
      "score1_neg              -0.067886       -0.071978           -0.073413   \n",
      "score2_pos               0.031040        0.005616            0.027354   \n",
      "score2_neg               0.032101        0.049560            0.042525   \n",
      "score3_pos              -0.030930       -0.142681            0.016359   \n",
      "score3_neg               0.132938       -0.040057            0.038332   \n",
      "score4_pos               0.022998       -0.062394            0.051443   \n",
      "score4_neg               0.001900       -0.047304           -0.001732   \n",
      "score5_pos               0.366369        0.062191           -0.018134   \n",
      "score5_neg               1.000000        0.058454            0.047610   \n",
      "outcome_profit           0.058454        1.000000            0.010217   \n",
      "outcome_damage_inc       0.047610        0.010217            1.000000   \n",
      "outcome_damage_amount    0.043855        0.044915            0.855924   \n",
      "\n",
      "                       outcome_damage_amount  \n",
      "income_am                           0.068846  \n",
      "profit_last_am                      0.065361  \n",
      "profit_am                           0.073691  \n",
      "damage_am                           0.125705  \n",
      "damage_inc                          0.128011  \n",
      "crd_lim_rec                         0.104761  \n",
      "credit_use_ic                       0.092227  \n",
      "gluten_ic                           0.038610  \n",
      "lactose_ic                          0.083122  \n",
      "insurance_ic                        0.114252  \n",
      "spa_ic                              0.007221  \n",
      "empl_ic                            -0.009774  \n",
      "cab_requests                        0.156747  \n",
      "married_cd                          0.031207  \n",
      "bar_no                              0.249577  \n",
      "sport_ic                            0.072809  \n",
      "neighbor_income                     0.039387  \n",
      "age                                -0.020853  \n",
      "marketing_permit                    0.078196  \n",
      "urban_ic                            0.019570  \n",
      "dining_ic                           0.024262  \n",
      "presidential                        0.032031  \n",
      "client_segment                      0.106070  \n",
      "sect_empl                           0.072243  \n",
      "prev_stay                           0.009207  \n",
      "prev_all_in_stay                    0.176143  \n",
      "divorce                            -0.020929  \n",
      "fam_adult_size                      0.064025  \n",
      "children_no                         0.039779  \n",
      "tenure_mts                          0.011896  \n",
      "tenure_yrs                          0.011605  \n",
      "company_ic                          0.033828  \n",
      "claims_no                           0.022110  \n",
      "claims_am                           0.008285  \n",
      "nights_booked                       0.053562  \n",
      "shop_am                             0.049261  \n",
      "shop_use                            0.054321  \n",
      "retired                            -0.024040  \n",
      "gold_status                         0.072045  \n",
      "score1_pos                          0.029736  \n",
      "score1_neg                         -0.028988  \n",
      "score2_pos                          0.050933  \n",
      "score2_neg                          0.075848  \n",
      "score3_pos                         -0.016141  \n",
      "score3_neg                          0.014149  \n",
      "score4_pos                          0.038020  \n",
      "score4_neg                         -0.011870  \n",
      "score5_pos                         -0.002802  \n",
      "score5_neg                          0.043855  \n",
      "outcome_profit                      0.044915  \n",
      "outcome_damage_inc                  0.855924  \n",
      "outcome_damage_amount               1.000000  \n",
      "\n",
      "[52 rows x 52 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joery\\AppData\\Local\\Temp\\ipykernel_8704\\449586712.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_matrix = train.corr()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# correlation matrix\n",
    "corr_matrix = train.corr()\n",
    "print(corr_matrix)\n",
    "# Geen variabelen die een correlatie van 1 hebben dus op basis daarvan moeten we geen variabelen weglaten.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# There are no constant variables so we do not need to ommit any based on this information.\n",
    "constant_columns = [col for col in train.columns if train[col].nunique() == 1]\n",
    "print(constant_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>income_am</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>2281.260158</td>\n",
       "      <td>8365.254507</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>1688.000000</td>\n",
       "      <td>360577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_last_am</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>696.057712</td>\n",
       "      <td>3051.119275</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>150537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_am</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>3637.900950</td>\n",
       "      <td>5726.625669</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1638.000000</td>\n",
       "      <td>1889.000000</td>\n",
       "      <td>3165.500000</td>\n",
       "      <td>100577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage_am</th>\n",
       "      <td>4954.0</td>\n",
       "      <td>145.952967</td>\n",
       "      <td>581.068095</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14866.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage_inc</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.352335</td>\n",
       "      <td>0.889449</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crd_lim_rec</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>3298.716394</td>\n",
       "      <td>4549.646039</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_use_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.198858</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluten_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.024661</td>\n",
       "      <td>0.155107</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lactose_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.292134</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insurance_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.390944</td>\n",
       "      <td>0.488011</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa_ic</th>\n",
       "      <td>4970.0</td>\n",
       "      <td>0.401811</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>empl_ic</th>\n",
       "      <td>4999.0</td>\n",
       "      <td>0.024205</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cab_requests</th>\n",
       "      <td>4912.0</td>\n",
       "      <td>6.051507</td>\n",
       "      <td>3.112104</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bar_no</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>5.646250</td>\n",
       "      <td>5.052513</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.287043</td>\n",
       "      <td>0.452427</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbor_income</th>\n",
       "      <td>4761.0</td>\n",
       "      <td>32778.558916</td>\n",
       "      <td>6858.671948</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>28630.000000</td>\n",
       "      <td>31990.000000</td>\n",
       "      <td>35924.000000</td>\n",
       "      <td>104984.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>44.901152</td>\n",
       "      <td>16.225094</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing_permit</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.495452</td>\n",
       "      <td>0.500030</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urban_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.883970</td>\n",
       "      <td>0.320293</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dining_ic</th>\n",
       "      <td>4912.0</td>\n",
       "      <td>0.049267</td>\n",
       "      <td>0.216447</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidential</th>\n",
       "      <td>4912.0</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.065252</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_segment</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>1.298565</td>\n",
       "      <td>0.800831</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sect_empl</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.213463</td>\n",
       "      <td>0.826006</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_stay</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.889832</td>\n",
       "      <td>0.313130</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_all_in_stay</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.252678</td>\n",
       "      <td>0.434592</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.102486</td>\n",
       "      <td>0.303317</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fam_adult_size</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>1.960986</td>\n",
       "      <td>0.805545</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children_no</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.385082</td>\n",
       "      <td>0.832933</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure_mts</th>\n",
       "      <td>4608.0</td>\n",
       "      <td>273.111545</td>\n",
       "      <td>152.498416</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>368.250000</td>\n",
       "      <td>679.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure_yrs</th>\n",
       "      <td>4608.0</td>\n",
       "      <td>22.780165</td>\n",
       "      <td>12.719429</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_ic</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.018597</td>\n",
       "      <td>0.135111</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claims_no</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.218314</td>\n",
       "      <td>0.712408</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claims_am</th>\n",
       "      <td>4973.0</td>\n",
       "      <td>121.078826</td>\n",
       "      <td>1783.146726</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nights_booked</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>28.992521</td>\n",
       "      <td>37.480510</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>375.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_am</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>403.019960</td>\n",
       "      <td>1335.935144</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12098.364339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_use</th>\n",
       "      <td>4912.0</td>\n",
       "      <td>0.151873</td>\n",
       "      <td>0.358934</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retired</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.182131</td>\n",
       "      <td>0.385991</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold_status</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>0.183212</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score1_pos</th>\n",
       "      <td>1225.0</td>\n",
       "      <td>0.499736</td>\n",
       "      <td>0.287925</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.252021</td>\n",
       "      <td>0.497416</td>\n",
       "      <td>0.748728</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score1_neg</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>0.500366</td>\n",
       "      <td>0.288717</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.251034</td>\n",
       "      <td>0.498621</td>\n",
       "      <td>0.751673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score2_pos</th>\n",
       "      <td>1209.0</td>\n",
       "      <td>0.498552</td>\n",
       "      <td>0.287757</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.252128</td>\n",
       "      <td>0.498779</td>\n",
       "      <td>0.744140</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score2_neg</th>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.496734</td>\n",
       "      <td>0.289799</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.245421</td>\n",
       "      <td>0.498583</td>\n",
       "      <td>0.747493</td>\n",
       "      <td>0.998651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score3_pos</th>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.494280</td>\n",
       "      <td>0.289917</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.240557</td>\n",
       "      <td>0.494246</td>\n",
       "      <td>0.744923</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score3_neg</th>\n",
       "      <td>1367.0</td>\n",
       "      <td>0.498588</td>\n",
       "      <td>0.287729</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.249506</td>\n",
       "      <td>0.501646</td>\n",
       "      <td>0.746483</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score4_pos</th>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.496206</td>\n",
       "      <td>0.288654</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.247410</td>\n",
       "      <td>0.493349</td>\n",
       "      <td>0.745213</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score4_neg</th>\n",
       "      <td>1324.0</td>\n",
       "      <td>0.501396</td>\n",
       "      <td>0.287623</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.250670</td>\n",
       "      <td>0.502060</td>\n",
       "      <td>0.749388</td>\n",
       "      <td>0.999312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score5_pos</th>\n",
       "      <td>1232.0</td>\n",
       "      <td>0.500959</td>\n",
       "      <td>0.290132</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.251490</td>\n",
       "      <td>0.502912</td>\n",
       "      <td>0.751282</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score5_neg</th>\n",
       "      <td>1493.0</td>\n",
       "      <td>5.192953</td>\n",
       "      <td>3.159868</td>\n",
       "      <td>-7.871775e+00</td>\n",
       "      <td>3.124958</td>\n",
       "      <td>5.188006</td>\n",
       "      <td>7.357425</td>\n",
       "      <td>14.776319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_profit</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1967.310930</td>\n",
       "      <td>1371.061266</td>\n",
       "      <td>1.068000e+01</td>\n",
       "      <td>1333.320000</td>\n",
       "      <td>1721.235000</td>\n",
       "      <td>2223.712500</td>\n",
       "      <td>31529.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_damage_inc</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.436129</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_damage_amount</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>189.970736</td>\n",
       "      <td>379.005941</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>202.612500</td>\n",
       "      <td>3157.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count          mean          std           min  \\\n",
       "income_am              4947.0   2281.260158  8365.254507  0.000000e+00   \n",
       "profit_last_am         4947.0    696.057712  3051.119275  0.000000e+00   \n",
       "profit_am              4947.0   3637.900950  5726.625669  0.000000e+00   \n",
       "damage_am              4954.0    145.952967   581.068095  0.000000e+00   \n",
       "damage_inc             4947.0      0.352335     0.889449  0.000000e+00   \n",
       "crd_lim_rec            4947.0   3298.716394  4549.646039  0.000000e+00   \n",
       "credit_use_ic          4947.0      0.041237     0.198858  0.000000e+00   \n",
       "gluten_ic              4947.0      0.024661     0.155107  0.000000e+00   \n",
       "lactose_ic             4947.0      0.094199     0.292134  0.000000e+00   \n",
       "insurance_ic           4947.0      0.390944     0.488011  0.000000e+00   \n",
       "spa_ic                 4970.0      0.401811     0.490313  0.000000e+00   \n",
       "empl_ic                4999.0      0.024205     0.153700  0.000000e+00   \n",
       "cab_requests           4912.0      6.051507     3.112104  0.000000e+00   \n",
       "bar_no                 4947.0      5.646250     5.052513  0.000000e+00   \n",
       "sport_ic               4947.0      0.287043     0.452427  0.000000e+00   \n",
       "neighbor_income        4761.0  32778.558916  6858.671948  0.000000e+00   \n",
       "age                    4947.0     44.901152    16.225094  1.600000e+01   \n",
       "marketing_permit       4947.0      0.495452     0.500030  0.000000e+00   \n",
       "urban_ic               4947.0      0.883970     0.320293  0.000000e+00   \n",
       "dining_ic              4912.0      0.049267     0.216447  0.000000e+00   \n",
       "presidential           4912.0      0.004275     0.065252  0.000000e+00   \n",
       "client_segment         4947.0      1.298565     0.800831  0.000000e+00   \n",
       "sect_empl              4947.0      0.213463     0.826006  0.000000e+00   \n",
       "prev_stay              4947.0      0.889832     0.313130  0.000000e+00   \n",
       "prev_all_in_stay       4947.0      0.252678     0.434592  0.000000e+00   \n",
       "divorce                4947.0      0.102486     0.303317  0.000000e+00   \n",
       "fam_adult_size         4947.0      1.960986     0.805545  1.000000e+00   \n",
       "children_no            4947.0      0.385082     0.832933  0.000000e+00   \n",
       "tenure_mts             4608.0    273.111545   152.498416  0.000000e+00   \n",
       "tenure_yrs             4608.0     22.780165    12.719429  0.000000e+00   \n",
       "company_ic             4947.0      0.018597     0.135111  0.000000e+00   \n",
       "claims_no              4947.0      0.218314     0.712408  0.000000e+00   \n",
       "claims_am              4973.0    121.078826  1783.146726  0.000000e+00   \n",
       "nights_booked          4947.0     28.992521    37.480510  1.000000e+00   \n",
       "shop_am                4947.0    403.019960  1335.935144  0.000000e+00   \n",
       "shop_use               4912.0      0.151873     0.358934  0.000000e+00   \n",
       "retired                4947.0      0.182131     0.385991  0.000000e+00   \n",
       "gold_status            4947.0      0.034769     0.183212  0.000000e+00   \n",
       "score1_pos             1225.0      0.499736     0.287925  1.000000e-07   \n",
       "score1_neg             1314.0      0.500366     0.288717  1.000000e-07   \n",
       "score2_pos             1209.0      0.498552     0.287757  1.000000e-07   \n",
       "score2_neg             1304.0      0.496734     0.289799  1.000000e-07   \n",
       "score3_pos             1261.0      0.494280     0.289917  1.000000e-07   \n",
       "score3_neg             1367.0      0.498588     0.287729  1.000000e-07   \n",
       "score4_pos             1223.0      0.496206     0.288654  1.000000e-07   \n",
       "score4_neg             1324.0      0.501396     0.287623  1.000000e-07   \n",
       "score5_pos             1232.0      0.500959     0.290132  1.000000e-07   \n",
       "score5_neg             1493.0      5.192953     3.159868 -7.871775e+00   \n",
       "outcome_profit         5000.0   1967.310930  1371.061266  1.068000e+01   \n",
       "outcome_damage_inc     5000.0      0.255400     0.436129  0.000000e+00   \n",
       "outcome_damage_amount  5000.0    189.970736   379.005941  0.000000e+00   \n",
       "\n",
       "                                25%           50%           75%            max  \n",
       "income_am                229.000000    469.000000   1688.000000  360577.000000  \n",
       "profit_last_am             0.000000     52.000000    810.000000  150537.000000  \n",
       "profit_am               1638.000000   1889.000000   3165.500000  100577.000000  \n",
       "damage_am                  0.000000      0.000000      0.000000   14866.000000  \n",
       "damage_inc                 0.000000      0.000000      0.000000      10.000000  \n",
       "crd_lim_rec                0.000000   1500.000000   5000.000000   30000.000000  \n",
       "credit_use_ic              0.000000      0.000000      0.000000       1.000000  \n",
       "gluten_ic                  0.000000      0.000000      0.000000       1.000000  \n",
       "lactose_ic                 0.000000      0.000000      0.000000       1.000000  \n",
       "insurance_ic               0.000000      0.000000      1.000000       1.000000  \n",
       "spa_ic                     0.000000      0.000000      1.000000       1.000000  \n",
       "empl_ic                    0.000000      0.000000      0.000000       1.000000  \n",
       "cab_requests               3.000000      6.000000      9.000000      16.000000  \n",
       "bar_no                     2.000000      5.000000      8.000000     111.000000  \n",
       "sport_ic                   0.000000      0.000000      1.000000       1.000000  \n",
       "neighbor_income        28630.000000  31990.000000  35924.000000  104984.000000  \n",
       "age                       31.000000     45.000000     57.000000      97.000000  \n",
       "marketing_permit           0.000000      0.000000      1.000000       1.000000  \n",
       "urban_ic                   1.000000      1.000000      1.000000       1.000000  \n",
       "dining_ic                  0.000000      0.000000      0.000000       1.000000  \n",
       "presidential               0.000000      0.000000      0.000000       1.000000  \n",
       "client_segment             1.000000      1.000000      2.000000       5.000000  \n",
       "sect_empl                  0.000000      0.000000      0.000000       6.000000  \n",
       "prev_stay                  1.000000      1.000000      1.000000       1.000000  \n",
       "prev_all_in_stay           0.000000      0.000000      1.000000       1.000000  \n",
       "divorce                    0.000000      0.000000      0.000000       1.000000  \n",
       "fam_adult_size             1.000000      2.000000      3.000000       4.000000  \n",
       "children_no                0.000000      0.000000      0.000000       6.000000  \n",
       "tenure_mts               154.000000    271.000000    368.250000     679.000000  \n",
       "tenure_yrs                13.000000     23.000000     31.000000      57.000000  \n",
       "company_ic                 0.000000      0.000000      0.000000       1.000000  \n",
       "claims_no                  0.000000      0.000000      0.000000       9.000000  \n",
       "claims_am                  0.000000      0.000000      0.000000   90587.000000  \n",
       "nights_booked              4.000000     11.000000     45.000000     375.000000  \n",
       "shop_am                    0.000000      0.000000      0.000000   12098.364339  \n",
       "shop_use                   0.000000      0.000000      0.000000       1.000000  \n",
       "retired                    0.000000      0.000000      0.000000       1.000000  \n",
       "gold_status                0.000000      0.000000      0.000000       1.000000  \n",
       "score1_pos                 0.252021      0.497416      0.748728       1.000000  \n",
       "score1_neg                 0.251034      0.498621      0.751673       1.000000  \n",
       "score2_pos                 0.252128      0.498779      0.744140       1.000000  \n",
       "score2_neg                 0.245421      0.498583      0.747493       0.998651  \n",
       "score3_pos                 0.240557      0.494246      0.744923       1.000000  \n",
       "score3_neg                 0.249506      0.501646      0.746483       1.000000  \n",
       "score4_pos                 0.247410      0.493349      0.745213       1.000000  \n",
       "score4_neg                 0.250670      0.502060      0.749388       0.999312  \n",
       "score5_pos                 0.251490      0.502912      0.751282       1.000000  \n",
       "score5_neg                 3.124958      5.188006      7.357425      14.776319  \n",
       "outcome_profit          1333.320000   1721.235000   2223.712500   31529.000000  \n",
       "outcome_damage_inc         0.000000      0.000000      1.000000       1.000000  \n",
       "outcome_damage_amount      0.000000      0.000000    202.612500    3157.240000  "
      ]
     },
     "execution_count": 1217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>income_am</th>\n",
       "      <td>227.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>4091.0</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>13293.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.0</td>\n",
       "      <td>3655.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3611.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_last_am</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2656.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profit_am</th>\n",
       "      <td>3201.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>6280.0</td>\n",
       "      <td>14704.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1630.0</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>2181.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>5313.0</td>\n",
       "      <td>7426.0</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>1771.0</td>\n",
       "      <td>1910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage_am</th>\n",
       "      <td>888.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage_inc</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crd_lim_rec</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_use_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gluten_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lactose_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insurance_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa_ic</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>empl_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cab_requests</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>married_cd</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bar_no</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport_ic</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbor_income</th>\n",
       "      <td>28936.0</td>\n",
       "      <td>16674.0</td>\n",
       "      <td>32552.0</td>\n",
       "      <td>32252.0</td>\n",
       "      <td>29605.0</td>\n",
       "      <td>27621.0</td>\n",
       "      <td>33459.0</td>\n",
       "      <td>29087.0</td>\n",
       "      <td>33508.0</td>\n",
       "      <td>31718.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26463.0</td>\n",
       "      <td>28455.0</td>\n",
       "      <td>55320.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38795.0</td>\n",
       "      <td>32618.0</td>\n",
       "      <td>36047.0</td>\n",
       "      <td>34202.0</td>\n",
       "      <td>27457.0</td>\n",
       "      <td>35838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>37.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing_permit</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urban_ic</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dining_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidential</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_segment</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sect_empl</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_stay</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_all_in_stay</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fam_adult_size</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children_no</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure_mts</th>\n",
       "      <td>476.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>354.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>283.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure_yrs</th>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_ic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claims_no</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claims_am</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nights_booked</th>\n",
       "      <td>209.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_am</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1454.210627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6602.744412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_use</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retired</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold_status</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score1_pos</th>\n",
       "      <td>0.467768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12786</td>\n",
       "      <td>0.985208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.643764</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score1_neg</th>\n",
       "      <td>0.98334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.932476</td>\n",
       "      <td>0.961357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69745</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score2_pos</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030314</td>\n",
       "      <td>0.834206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.376229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score2_neg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.901703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.203528</td>\n",
       "      <td>0.595094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.533381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.265279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score3_pos</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330503</td>\n",
       "      <td>0.421064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.196373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634899</td>\n",
       "      <td>0.184069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.755112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score3_neg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.889793</td>\n",
       "      <td>0.766294</td>\n",
       "      <td>0.584441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.318601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541089</td>\n",
       "      <td>0.336643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.451572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score4_pos</th>\n",
       "      <td>0.838147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.490486</td>\n",
       "      <td>0.872812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score4_neg</th>\n",
       "      <td>0.082288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.542445</td>\n",
       "      <td>0.668272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.316801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score5_pos</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817945</td>\n",
       "      <td>0.461859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score5_neg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.955259</td>\n",
       "      <td>1.74302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.894609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.672416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.372577</td>\n",
       "      <td>3.878867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_profit</th>\n",
       "      <td>1791.66</td>\n",
       "      <td>1672.78</td>\n",
       "      <td>1001.4</td>\n",
       "      <td>1785.59</td>\n",
       "      <td>3140.74</td>\n",
       "      <td>4318.76</td>\n",
       "      <td>1676.75</td>\n",
       "      <td>1127.05</td>\n",
       "      <td>1640.31</td>\n",
       "      <td>1861.78</td>\n",
       "      <td>...</td>\n",
       "      <td>1665.64</td>\n",
       "      <td>1184.51</td>\n",
       "      <td>1839.31</td>\n",
       "      <td>1977.6</td>\n",
       "      <td>1252.76</td>\n",
       "      <td>2611.49</td>\n",
       "      <td>4590.76</td>\n",
       "      <td>1379.2</td>\n",
       "      <td>988.35</td>\n",
       "      <td>880.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_damage_inc</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome_damage_amount</th>\n",
       "      <td>0.0</td>\n",
       "      <td>829.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>888.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>981.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>613.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1419.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3            4    \\\n",
       "income_am                 227.0     268.0     283.0     227.0       4091.0   \n",
       "profit_last_am              0.0      16.0      23.0       0.0       1028.0   \n",
       "profit_am                3201.0    1682.0    1673.0    1685.0       3425.0   \n",
       "damage_am                 888.0       0.0       0.0       0.0        785.0   \n",
       "damage_inc                  6.0       0.0       0.0       0.0          2.0   \n",
       "crd_lim_rec             15000.0     750.0     750.0       0.0      14000.0   \n",
       "credit_use_ic               0.0       0.0       0.0       0.0          0.0   \n",
       "gluten_ic                   0.0       0.0       0.0       0.0          0.0   \n",
       "lactose_ic                  0.0       0.0       0.0       0.0          1.0   \n",
       "insurance_ic                0.0       1.0       1.0       0.0          0.0   \n",
       "spa_ic                      1.0       1.0       0.0       0.0          1.0   \n",
       "empl_ic                     0.0       0.0       0.0       0.0          0.0   \n",
       "cab_requests                3.0       7.0       1.0       6.0          4.0   \n",
       "married_cd                 True      True      True      True        False   \n",
       "bar_no                      2.0       3.0       4.0       8.0          2.0   \n",
       "sport_ic                    1.0       0.0       0.0       1.0          1.0   \n",
       "neighbor_income         28936.0   16674.0   32552.0   32252.0      29605.0   \n",
       "age                        37.0      18.0      21.0      37.0         26.0   \n",
       "marketing_permit            0.0       0.0       0.0       0.0          0.0   \n",
       "urban_ic                    1.0       0.0       1.0       1.0          1.0   \n",
       "dining_ic                   0.0       0.0       0.0       0.0          0.0   \n",
       "presidential                0.0       0.0       0.0       0.0          0.0   \n",
       "client_segment              1.0       1.0       1.0       1.0          2.0   \n",
       "sect_empl                   1.0       0.0       0.0       0.0          0.0   \n",
       "prev_stay                   1.0       0.0       1.0       1.0          1.0   \n",
       "prev_all_in_stay            1.0       0.0       0.0       0.0          0.0   \n",
       "divorce                     0.0       0.0       0.0       0.0          0.0   \n",
       "fam_adult_size              3.0       1.0       1.0       3.0          2.0   \n",
       "children_no                 2.0       0.0       0.0       2.0          0.0   \n",
       "tenure_mts                476.0      27.0      95.0       NaN        354.0   \n",
       "tenure_yrs                 40.0       2.0       8.0       NaN         30.0   \n",
       "company_ic                  0.0       0.0       0.0       0.0          0.0   \n",
       "claims_no                   0.0       0.0       0.0       0.0          0.0   \n",
       "claims_am                   0.0       0.0       0.0       0.0          0.0   \n",
       "nights_booked             209.0       4.0       6.0       4.0          3.0   \n",
       "gender                        M         M         M         V            V   \n",
       "shop_am                     0.0       0.0       0.0       0.0  1454.210627   \n",
       "shop_use                    0.0       0.0       0.0       0.0          1.0   \n",
       "retired                     0.0       0.0       0.0       0.0          0.0   \n",
       "gold_status                 0.0       0.0       0.0       0.0          0.0   \n",
       "score1_pos             0.467768       NaN       NaN       NaN          NaN   \n",
       "score1_neg              0.98334       NaN       NaN       NaN          NaN   \n",
       "score2_pos                  NaN       NaN  0.232375       NaN          NaN   \n",
       "score2_neg                  NaN       NaN  0.099529       NaN          NaN   \n",
       "score3_pos                  NaN       NaN       NaN       NaN     0.330503   \n",
       "score3_neg                  NaN       NaN       NaN  0.889793     0.766294   \n",
       "score4_pos             0.838147       NaN       NaN       NaN     0.490486   \n",
       "score4_neg             0.082288       NaN       NaN       NaN     0.542445   \n",
       "score5_pos                  NaN       NaN  0.101955       NaN          NaN   \n",
       "score5_neg                  NaN  7.955259   1.74302       NaN          NaN   \n",
       "outcome_profit          1791.66   1672.78    1001.4   1785.59      3140.74   \n",
       "outcome_damage_inc            0         1         0         0            0   \n",
       "outcome_damage_amount       0.0    829.66       0.0       0.0          0.0   \n",
       "\n",
       "                            5         6        7         8         9    ...  \\\n",
       "income_am                5108.0   13293.0    227.0     229.0     229.0  ...   \n",
       "profit_last_am            289.0      28.0      0.0     810.0     810.0  ...   \n",
       "profit_am                6280.0   14704.0   1620.0    1620.0    1620.0  ...   \n",
       "damage_am                   0.0       0.0      0.0       0.0       0.0  ...   \n",
       "damage_inc                  0.0       0.0      0.0       0.0       0.0  ...   \n",
       "crd_lim_rec                 0.0   15000.0      0.0       0.0       0.0  ...   \n",
       "credit_use_ic               0.0       0.0      0.0       0.0       0.0  ...   \n",
       "gluten_ic                   0.0       1.0      0.0       0.0       0.0  ...   \n",
       "lactose_ic                  0.0       1.0      0.0       0.0       0.0  ...   \n",
       "insurance_ic                0.0       1.0      0.0       1.0       1.0  ...   \n",
       "spa_ic                      0.0       1.0      0.0       0.0       0.0  ...   \n",
       "empl_ic                     0.0       0.0      0.0       0.0       0.0  ...   \n",
       "cab_requests                3.0       8.0      NaN       3.0       8.0  ...   \n",
       "married_cd                False      True     True      True      True  ...   \n",
       "bar_no                      6.0      18.0      9.0       6.0       6.0  ...   \n",
       "sport_ic                    0.0       0.0      0.0       0.0       0.0  ...   \n",
       "neighbor_income         27621.0   33459.0  29087.0   33508.0   31718.0  ...   \n",
       "age                        84.0      46.0     56.0      33.0      29.0  ...   \n",
       "marketing_permit            0.0       0.0      0.0       0.0       0.0  ...   \n",
       "urban_ic                    1.0       1.0      1.0       1.0       1.0  ...   \n",
       "dining_ic                   0.0       0.0      NaN       0.0       0.0  ...   \n",
       "presidential                0.0       0.0      NaN       0.0       0.0  ...   \n",
       "client_segment              2.0       3.0      0.0       0.0       1.0  ...   \n",
       "sect_empl                   0.0       0.0      0.0       0.0       0.0  ...   \n",
       "prev_stay                   1.0       1.0      1.0       0.0       1.0  ...   \n",
       "prev_all_in_stay            0.0       1.0      0.0       0.0       0.0  ...   \n",
       "divorce                     0.0       0.0      0.0       0.0       0.0  ...   \n",
       "fam_adult_size              1.0       3.0      2.0       2.0       2.0  ...   \n",
       "children_no                 0.0       2.0      0.0       0.0       0.0  ...   \n",
       "tenure_mts                350.0     324.0      NaN       1.0     145.0  ...   \n",
       "tenure_yrs                 29.0      27.0      NaN       0.0      12.0  ...   \n",
       "company_ic                  0.0       0.0      0.0       0.0       0.0  ...   \n",
       "claims_no                   0.0       0.0      0.0       0.0       0.0  ...   \n",
       "claims_am                   0.0       0.0      0.0       0.0       0.0  ...   \n",
       "nights_booked               4.0      82.0      4.0       4.0       4.0  ...   \n",
       "gender                        V         M        V         M         M  ...   \n",
       "shop_am                     0.0       0.0      0.0       0.0       0.0  ...   \n",
       "shop_use                    0.0       0.0      NaN       0.0       0.0  ...   \n",
       "retired                     1.0       0.0      0.0       0.0       0.0  ...   \n",
       "gold_status                 1.0       1.0      0.0       0.0       0.0  ...   \n",
       "score1_pos                  NaN  0.566906      NaN       NaN       NaN  ...   \n",
       "score1_neg                  NaN   0.84632      NaN       NaN       NaN  ...   \n",
       "score2_pos                  NaN  0.595743      NaN       NaN       NaN  ...   \n",
       "score2_neg                  NaN  0.901703      NaN       NaN       NaN  ...   \n",
       "score3_pos             0.421064       NaN      NaN       NaN  0.196373  ...   \n",
       "score3_neg             0.584441       NaN      NaN       NaN  0.318601  ...   \n",
       "score4_pos             0.872812       NaN      NaN       NaN       NaN  ...   \n",
       "score4_neg             0.668272       NaN      NaN       NaN       NaN  ...   \n",
       "score5_pos                  NaN       NaN      NaN       NaN       NaN  ...   \n",
       "score5_neg                  NaN       NaN      NaN  6.894609       NaN  ...   \n",
       "outcome_profit          4318.76   1676.75  1127.05   1640.31   1861.78  ...   \n",
       "outcome_damage_inc            0         1        0         0         1  ...   \n",
       "outcome_damage_amount       0.0    888.09      0.0       0.0    981.64  ...   \n",
       "\n",
       "                            490       491       492     493       494  \\\n",
       "income_am                 275.0    3655.0     227.0  3611.0     227.0   \n",
       "profit_last_am             20.0    1441.0       0.0  1371.0       0.0   \n",
       "profit_am                1630.0    2185.0    1620.0  2181.0    1620.0   \n",
       "damage_am                   0.0       0.0       0.0     0.0       0.0   \n",
       "damage_inc                  0.0       0.0       0.0     0.0       0.0   \n",
       "crd_lim_rec                 0.0       0.0   15000.0  4000.0       0.0   \n",
       "credit_use_ic               0.0       0.0       0.0     0.0       0.0   \n",
       "gluten_ic                   0.0       0.0       0.0     0.0       0.0   \n",
       "lactose_ic                  0.0       0.0       0.0     0.0       0.0   \n",
       "insurance_ic                0.0       0.0       0.0     0.0       0.0   \n",
       "spa_ic                      0.0       0.0       0.0     0.0       0.0   \n",
       "empl_ic                     1.0       0.0       0.0     0.0       0.0   \n",
       "cab_requests               10.0       9.0       7.0     3.0       4.0   \n",
       "married_cd                 True      True      True   False     False   \n",
       "bar_no                      7.0       5.0       4.0     5.0       2.0   \n",
       "sport_ic                    0.0       0.0       0.0     0.0       0.0   \n",
       "neighbor_income         26463.0   28455.0   55320.0     NaN   38795.0   \n",
       "age                        62.0      53.0      37.0    61.0      41.0   \n",
       "marketing_permit            0.0       0.0       0.0     1.0       0.0   \n",
       "urban_ic                    1.0       1.0       1.0     1.0       1.0   \n",
       "dining_ic                   0.0       0.0       0.0     0.0       0.0   \n",
       "presidential                0.0       0.0       0.0     0.0       0.0   \n",
       "client_segment              1.0       2.0       1.0     1.0       1.0   \n",
       "sect_empl                   0.0       0.0       0.0     0.0       0.0   \n",
       "prev_stay                   1.0       1.0       1.0     1.0       1.0   \n",
       "prev_all_in_stay            0.0       0.0       0.0     0.0       0.0   \n",
       "divorce                     0.0       0.0       0.0     0.0       1.0   \n",
       "fam_adult_size              1.0       2.0       2.0     1.0       2.0   \n",
       "children_no                 0.0       0.0       3.0     0.0       0.0   \n",
       "tenure_mts                283.0     384.0     228.0   350.0     386.0   \n",
       "tenure_yrs                 24.0      32.0      19.0    29.0      32.0   \n",
       "company_ic                  0.0       0.0       0.0     0.0       0.0   \n",
       "claims_no                   0.0       0.0       0.0     0.0       0.0   \n",
       "claims_am                   0.0       0.0       0.0     0.0       0.0   \n",
       "nights_booked               3.0       4.0      26.0    34.0       4.0   \n",
       "gender                        M         V         V       V         V   \n",
       "shop_am                     0.0       0.0       0.0     0.0       0.0   \n",
       "shop_use                    0.0       0.0       0.0     0.0       0.0   \n",
       "retired                     0.0       0.0       0.0     1.0       0.0   \n",
       "gold_status                 0.0       0.0       0.0     0.0       0.0   \n",
       "score1_pos                  NaN       NaN       NaN     NaN       NaN   \n",
       "score1_neg                  NaN       NaN       NaN     NaN       NaN   \n",
       "score2_pos                  NaN  0.030314  0.834206     NaN       NaN   \n",
       "score2_neg                  NaN  0.203528  0.595094     NaN       NaN   \n",
       "score3_pos             0.634899  0.184069       NaN     NaN  0.975329   \n",
       "score3_neg             0.541089  0.336643       NaN     NaN  0.451572   \n",
       "score4_pos                  NaN       NaN  0.410388     NaN       NaN   \n",
       "score4_neg                  NaN       NaN  0.316801     NaN       NaN   \n",
       "score5_pos                  NaN       NaN       NaN     NaN       NaN   \n",
       "score5_neg                  NaN       NaN       NaN     NaN       NaN   \n",
       "outcome_profit          1665.64   1184.51   1839.31  1977.6   1252.76   \n",
       "outcome_damage_inc            0         0         1       0         0   \n",
       "outcome_damage_amount       0.0       0.0    613.01     0.0       0.0   \n",
       "\n",
       "                               495       496       497       498       499  \n",
       "income_am                   1564.0    1928.0    1010.0    1186.0     227.0  \n",
       "profit_last_am              2656.5      20.0     243.0     403.0       0.0  \n",
       "profit_am                   5313.0    7426.0    1819.0    1771.0    1910.0  \n",
       "damage_am                      0.0       0.0       0.0       0.0       0.0  \n",
       "damage_inc                     2.0       0.0       0.0       0.0       0.0  \n",
       "crd_lim_rec                    0.0       0.0    7000.0    4500.0    4000.0  \n",
       "credit_use_ic                  0.0       0.0       0.0       0.0       0.0  \n",
       "gluten_ic                      0.0       0.0       0.0       0.0       0.0  \n",
       "lactose_ic                     0.0       0.0       0.0       0.0       0.0  \n",
       "insurance_ic                   0.0       0.0       1.0       0.0       1.0  \n",
       "spa_ic                         0.0       1.0       0.0       0.0       1.0  \n",
       "empl_ic                        0.0       0.0       0.0       0.0       0.0  \n",
       "cab_requests                   6.0       6.0       4.0       4.0       6.0  \n",
       "married_cd                    True     False     False      True      True  \n",
       "bar_no                         3.0       8.0       0.0       4.0       6.0  \n",
       "sport_ic                       1.0       0.0       0.0       0.0       0.0  \n",
       "neighbor_income            32618.0   36047.0   34202.0   27457.0   35838.0  \n",
       "age                           47.0      58.0      44.0      75.0      38.0  \n",
       "marketing_permit               1.0       1.0       1.0       1.0       1.0  \n",
       "urban_ic                       1.0       1.0       1.0       1.0       1.0  \n",
       "dining_ic                      0.0       0.0       0.0       0.0       0.0  \n",
       "presidential                   0.0       0.0       0.0       0.0       0.0  \n",
       "client_segment                 1.0       2.0       1.0       1.0       1.0  \n",
       "sect_empl                      1.0       0.0       0.0       0.0       0.0  \n",
       "prev_stay                      1.0       1.0       1.0       1.0       1.0  \n",
       "prev_all_in_stay               0.0       0.0       0.0       0.0       1.0  \n",
       "divorce                        0.0       1.0       1.0       0.0       0.0  \n",
       "fam_adult_size                 2.0       1.0       2.0       1.0       2.0  \n",
       "children_no                    0.0       0.0       1.0       0.0       0.0  \n",
       "tenure_mts                   381.0     384.0     482.0     239.0     133.0  \n",
       "tenure_yrs                    32.0      32.0      40.0      20.0      11.0  \n",
       "company_ic                     0.0       0.0       0.0       0.0       0.0  \n",
       "claims_no                      0.0       0.0       0.0       0.0       0.0  \n",
       "claims_am                      0.0       0.0       0.0       0.0       0.0  \n",
       "nights_booked                  4.0       3.0      25.0      51.0      29.0  \n",
       "gender                           M         V         V         M         M  \n",
       "shop_am                6602.744412       0.0       0.0       0.0       0.0  \n",
       "shop_use                       1.0       0.0       0.0       0.0       0.0  \n",
       "retired                        0.0       0.0       0.0       1.0       0.0  \n",
       "gold_status                    0.0       0.0       0.0       0.0       0.0  \n",
       "score1_pos                 0.12786  0.985208       NaN  0.643764       NaN  \n",
       "score1_neg                0.932476  0.961357       NaN   0.69745       NaN  \n",
       "score2_pos                0.376229       NaN  0.259667       NaN       NaN  \n",
       "score2_neg                0.533381       NaN  0.265279       NaN       NaN  \n",
       "score3_pos                     NaN       NaN       NaN       NaN  0.755112  \n",
       "score3_neg                     NaN       NaN       NaN       NaN  0.497016  \n",
       "score4_pos                     NaN  0.939674       NaN       NaN       NaN  \n",
       "score4_neg                     NaN  0.480619       NaN       NaN       NaN  \n",
       "score5_pos                     NaN  0.994833       NaN  0.817945  0.461859  \n",
       "score5_neg                     NaN  9.672416       NaN  5.372577  3.878867  \n",
       "outcome_profit             2611.49   4590.76    1379.2    988.35    880.65  \n",
       "outcome_damage_inc               0         0         0         1         0  \n",
       "outcome_damage_amount          0.0       0.0       0.0   1419.19       0.0  \n",
       "\n",
       "[53 rows x 500 columns]"
      ]
     },
     "execution_count": 1218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0:500].T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check for datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 53 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   income_am              4947 non-null   float64\n",
      " 1   profit_last_am         4947 non-null   float64\n",
      " 2   profit_am              4947 non-null   float64\n",
      " 3   damage_am              4954 non-null   float64\n",
      " 4   damage_inc             4947 non-null   float64\n",
      " 5   crd_lim_rec            4947 non-null   float64\n",
      " 6   credit_use_ic          4947 non-null   float64\n",
      " 7   gluten_ic              4947 non-null   float64\n",
      " 8   lactose_ic             4947 non-null   float64\n",
      " 9   insurance_ic           4947 non-null   float64\n",
      " 10  spa_ic                 4970 non-null   float64\n",
      " 11  empl_ic                4999 non-null   float64\n",
      " 12  cab_requests           4912 non-null   float64\n",
      " 13  married_cd             5000 non-null   bool   \n",
      " 14  bar_no                 4947 non-null   float64\n",
      " 15  sport_ic               4947 non-null   float64\n",
      " 16  neighbor_income        4761 non-null   float64\n",
      " 17  age                    4947 non-null   float64\n",
      " 18  marketing_permit       4947 non-null   float64\n",
      " 19  urban_ic               4947 non-null   float64\n",
      " 20  dining_ic              4912 non-null   float64\n",
      " 21  presidential           4912 non-null   float64\n",
      " 22  client_segment         4947 non-null   float64\n",
      " 23  sect_empl              4947 non-null   float64\n",
      " 24  prev_stay              4947 non-null   float64\n",
      " 25  prev_all_in_stay       4947 non-null   float64\n",
      " 26  divorce                4947 non-null   float64\n",
      " 27  fam_adult_size         4947 non-null   float64\n",
      " 28  children_no            4947 non-null   float64\n",
      " 29  tenure_mts             4608 non-null   float64\n",
      " 30  tenure_yrs             4608 non-null   float64\n",
      " 31  company_ic             4947 non-null   float64\n",
      " 32  claims_no              4947 non-null   float64\n",
      " 33  claims_am              4973 non-null   float64\n",
      " 34  nights_booked          4947 non-null   float64\n",
      " 35  gender                 4947 non-null   object \n",
      " 36  shop_am                4947 non-null   float64\n",
      " 37  shop_use               4912 non-null   float64\n",
      " 38  retired                4947 non-null   float64\n",
      " 39  gold_status            4947 non-null   float64\n",
      " 40  score1_pos             1225 non-null   float64\n",
      " 41  score1_neg             1314 non-null   float64\n",
      " 42  score2_pos             1209 non-null   float64\n",
      " 43  score2_neg             1304 non-null   float64\n",
      " 44  score3_pos             1261 non-null   float64\n",
      " 45  score3_neg             1367 non-null   float64\n",
      " 46  score4_pos             1223 non-null   float64\n",
      " 47  score4_neg             1324 non-null   float64\n",
      " 48  score5_pos             1232 non-null   float64\n",
      " 49  score5_neg             1493 non-null   float64\n",
      " 50  outcome_profit         5000 non-null   float64\n",
      " 51  outcome_damage_inc     5000 non-null   int64  \n",
      " 52  outcome_damage_amount  5000 non-null   float64\n",
      "dtypes: bool(1), float64(50), int64(1), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. and 5. Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_am             53\n",
       "profit_last_am        53\n",
       "profit_am             53\n",
       "damage_am             46\n",
       "damage_inc            53\n",
       "crd_lim_rec           53\n",
       "credit_use_ic         53\n",
       "gluten_ic             53\n",
       "lactose_ic            53\n",
       "insurance_ic          53\n",
       "spa_ic                30\n",
       "empl_ic                1\n",
       "cab_requests          88\n",
       "bar_no                53\n",
       "sport_ic              53\n",
       "neighbor_income      239\n",
       "age                   53\n",
       "marketing_permit      53\n",
       "urban_ic              53\n",
       "dining_ic             88\n",
       "presidential          88\n",
       "client_segment        53\n",
       "sect_empl             53\n",
       "prev_stay             53\n",
       "prev_all_in_stay      53\n",
       "divorce               53\n",
       "fam_adult_size        53\n",
       "children_no           53\n",
       "tenure_mts           392\n",
       "tenure_yrs           392\n",
       "company_ic            53\n",
       "claims_no             53\n",
       "claims_am             27\n",
       "nights_booked         53\n",
       "gender                53\n",
       "shop_am               53\n",
       "shop_use              88\n",
       "retired               53\n",
       "gold_status           53\n",
       "score1_pos          3775\n",
       "score1_neg          3686\n",
       "score2_pos          3791\n",
       "score2_neg          3696\n",
       "score3_pos          3739\n",
       "score3_neg          3633\n",
       "score4_pos          3777\n",
       "score4_neg          3676\n",
       "score5_pos          3768\n",
       "score5_neg          3507\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we check how many missing values we have per variable.\n",
    "\n",
    "train.isnull().sum()[train.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_am           98.94\n",
       "profit_last_am      98.94\n",
       "profit_am           98.94\n",
       "damage_am           99.08\n",
       "damage_inc          98.94\n",
       "crd_lim_rec         98.94\n",
       "credit_use_ic       98.94\n",
       "gluten_ic           98.94\n",
       "lactose_ic          98.94\n",
       "insurance_ic        98.94\n",
       "spa_ic              99.40\n",
       "empl_ic             99.98\n",
       "cab_requests        98.24\n",
       "bar_no              98.94\n",
       "sport_ic            98.94\n",
       "neighbor_income     95.22\n",
       "age                 98.94\n",
       "marketing_permit    98.94\n",
       "urban_ic            98.94\n",
       "dining_ic           98.24\n",
       "presidential        98.24\n",
       "client_segment      98.94\n",
       "sect_empl           98.94\n",
       "prev_stay           98.94\n",
       "prev_all_in_stay    98.94\n",
       "divorce             98.94\n",
       "fam_adult_size      98.94\n",
       "children_no         98.94\n",
       "tenure_mts          92.16\n",
       "tenure_yrs          92.16\n",
       "company_ic          98.94\n",
       "claims_no           98.94\n",
       "claims_am           99.46\n",
       "nights_booked       98.94\n",
       "gender              98.94\n",
       "shop_am             98.94\n",
       "shop_use            98.24\n",
       "retired             98.94\n",
       "gold_status         98.94\n",
       "score1_pos          24.50\n",
       "score1_neg          26.28\n",
       "score2_pos          24.18\n",
       "score2_neg          26.08\n",
       "score3_pos          25.22\n",
       "score3_neg          27.34\n",
       "score4_pos          24.46\n",
       "score4_neg          26.48\n",
       "score5_pos          24.64\n",
       "score5_neg          29.86\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we look at what percentage of the observations are not NaN per variable\n",
    "\n",
    "(5000- train.isnull().sum()[train.isnull().sum() != 0])/5000*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_am</th>\n",
       "      <th>profit_last_am</th>\n",
       "      <th>profit_am</th>\n",
       "      <th>damage_am</th>\n",
       "      <th>damage_inc</th>\n",
       "      <th>crd_lim_rec</th>\n",
       "      <th>credit_use_ic</th>\n",
       "      <th>gluten_ic</th>\n",
       "      <th>lactose_ic</th>\n",
       "      <th>insurance_ic</th>\n",
       "      <th>spa_ic</th>\n",
       "      <th>empl_ic</th>\n",
       "      <th>cab_requests</th>\n",
       "      <th>married_cd</th>\n",
       "      <th>bar_no</th>\n",
       "      <th>sport_ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3201.0</td>\n",
       "      <td>888.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4091.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income_am  profit_last_am  profit_am  damage_am  damage_inc  crd_lim_rec  \\\n",
       "0      227.0             0.0     3201.0      888.0         6.0      15000.0   \n",
       "1      268.0            16.0     1682.0        0.0         0.0        750.0   \n",
       "2      283.0            23.0     1673.0        0.0         0.0        750.0   \n",
       "3      227.0             0.0     1685.0        0.0         0.0          0.0   \n",
       "4     4091.0          1028.0     3425.0      785.0         2.0      14000.0   \n",
       "\n",
       "   credit_use_ic  gluten_ic  lactose_ic  insurance_ic  spa_ic  empl_ic  \\\n",
       "0            0.0        0.0         0.0           0.0     1.0      0.0   \n",
       "1            0.0        0.0         0.0           1.0     1.0      0.0   \n",
       "2            0.0        0.0         0.0           1.0     0.0      0.0   \n",
       "3            0.0        0.0         0.0           0.0     0.0      0.0   \n",
       "4            0.0        0.0         1.0           0.0     1.0      0.0   \n",
       "\n",
       "   cab_requests  married_cd  bar_no  sport_ic  \n",
       "0           3.0        True     2.0       1.0  \n",
       "1           7.0        True     3.0       0.0  \n",
       "2           1.0        True     4.0       0.0  \n",
       "3           6.0        True     8.0       1.0  \n",
       "4           4.0       False     2.0       1.0  "
      ]
     },
     "execution_count": 1222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we look at the first 16 variables\n",
    "train.iloc[:,0:16].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor_income</th>\n",
       "      <th>age</th>\n",
       "      <th>marketing_permit</th>\n",
       "      <th>urban_ic</th>\n",
       "      <th>dining_ic</th>\n",
       "      <th>presidential</th>\n",
       "      <th>client_segment</th>\n",
       "      <th>sect_empl</th>\n",
       "      <th>prev_stay</th>\n",
       "      <th>prev_all_in_stay</th>\n",
       "      <th>...</th>\n",
       "      <th>score2_neg</th>\n",
       "      <th>score3_pos</th>\n",
       "      <th>score3_neg</th>\n",
       "      <th>score4_pos</th>\n",
       "      <th>score4_neg</th>\n",
       "      <th>score5_pos</th>\n",
       "      <th>score5_neg</th>\n",
       "      <th>outcome_profit</th>\n",
       "      <th>outcome_damage_inc</th>\n",
       "      <th>outcome_damage_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28936.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838147</td>\n",
       "      <td>0.082288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1791.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16674.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.955259</td>\n",
       "      <td>1672.78</td>\n",
       "      <td>1</td>\n",
       "      <td>829.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32552.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>1.743020</td>\n",
       "      <td>1001.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32252.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.889793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1785.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29605.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330503</td>\n",
       "      <td>0.766294</td>\n",
       "      <td>0.490486</td>\n",
       "      <td>0.542445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3140.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   neighbor_income   age  marketing_permit  urban_ic  dining_ic  presidential  \\\n",
       "0          28936.0  37.0               0.0       1.0        0.0           0.0   \n",
       "1          16674.0  18.0               0.0       0.0        0.0           0.0   \n",
       "2          32552.0  21.0               0.0       1.0        0.0           0.0   \n",
       "3          32252.0  37.0               0.0       1.0        0.0           0.0   \n",
       "4          29605.0  26.0               0.0       1.0        0.0           0.0   \n",
       "\n",
       "   client_segment  sect_empl  prev_stay  prev_all_in_stay  ...  score2_neg  \\\n",
       "0             1.0        1.0        1.0               1.0  ...         NaN   \n",
       "1             1.0        0.0        0.0               0.0  ...         NaN   \n",
       "2             1.0        0.0        1.0               0.0  ...    0.099529   \n",
       "3             1.0        0.0        1.0               0.0  ...         NaN   \n",
       "4             2.0        0.0        1.0               0.0  ...         NaN   \n",
       "\n",
       "   score3_pos  score3_neg  score4_pos  score4_neg  score5_pos  score5_neg  \\\n",
       "0         NaN         NaN    0.838147    0.082288         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN    7.955259   \n",
       "2         NaN         NaN         NaN         NaN    0.101955    1.743020   \n",
       "3         NaN    0.889793         NaN         NaN         NaN         NaN   \n",
       "4    0.330503    0.766294    0.490486    0.542445         NaN         NaN   \n",
       "\n",
       "   outcome_profit  outcome_damage_inc outcome_damage_amount  \n",
       "0         1791.66                   0                  0.00  \n",
       "1         1672.78                   1                829.66  \n",
       "2         1001.40                   0                  0.00  \n",
       "3         1785.59                   0                  0.00  \n",
       "4         3140.74                   0                  0.00  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 1223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we look at the variables starting from the 16th just to see what the data looks like\n",
    "train.iloc[:,16:53].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count', ylabel='gender'>"
      ]
     },
     "execution_count": 1224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the amount of men and women with a bar chart\n",
    "sns.countplot(y=train[\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joery\\AppData\\Local\\Temp\\ipykernel_8704\\2512075102.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corrmat = train.corr()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrmat = train.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Add title to the Heat map\n",
    "title = \"Correlation between variables heatmap\"\n",
    "\n",
    "# Set the font size and the distance of the title from the plot\n",
    "plt.title(title,fontsize=18)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5,1.05])\n",
    "\n",
    "# Hide ticks for X & Y axis\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Remove the axes\n",
    "ax.axis('off')\n",
    "\n",
    "sns.heatmap(corrmat,fmt=\"\",cmap='RdYlGn',linewidths=0.30,ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_damage_inc\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_inc'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_profit\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_profit'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# features = train.copy()\n",
    "# features = features.drop([\"outcome_damage_amount\"], 1)\n",
    "# xvars = features.columns\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[0:5]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[5:10]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[10:15]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[15:20]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[20:25]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[25:30]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[30:35]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[35:40]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[40:45]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[45:50]))\n",
    "# sns.pairplot(train, y_vars=['outcome_damage_amount'], x_vars=(xvars[50:53]))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_damage_amount'] - train['outcome_damage_amount'].mean()) / train['outcome_damage_amount'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_damage_amount\"]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_profit'] - train['outcome_profit'].mean()) / train['outcome_profit'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_profit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# calculate the Z-score of each data point\n",
    "z_scores = np.abs((train['outcome_damage_inc'] - train['outcome_damage_inc'].mean()) / train['outcome_damage_inc'].std())\n",
    "\n",
    "# identify outliers as data points with a Z-score greater than 3\n",
    "outliers = train[z_scores > 3]\n",
    "\n",
    "# print the number of outliers\n",
    "print(len(outliers[\"outcome_damage_inc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 53 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   income_am              4947 non-null   float64\n",
      " 1   profit_last_am         4947 non-null   float64\n",
      " 2   profit_am              4947 non-null   float64\n",
      " 3   damage_am              4954 non-null   float64\n",
      " 4   damage_inc             4947 non-null   float64\n",
      " 5   crd_lim_rec            4947 non-null   float64\n",
      " 6   credit_use_ic          4947 non-null   float64\n",
      " 7   gluten_ic              4947 non-null   float64\n",
      " 8   lactose_ic             4947 non-null   float64\n",
      " 9   insurance_ic           4947 non-null   float64\n",
      " 10  spa_ic                 4970 non-null   float64\n",
      " 11  empl_ic                4999 non-null   float64\n",
      " 12  cab_requests           4912 non-null   float64\n",
      " 13  married_cd             5000 non-null   bool   \n",
      " 14  bar_no                 4947 non-null   float64\n",
      " 15  sport_ic               4947 non-null   float64\n",
      " 16  neighbor_income        4761 non-null   float64\n",
      " 17  age                    4947 non-null   float64\n",
      " 18  marketing_permit       4947 non-null   float64\n",
      " 19  urban_ic               4947 non-null   float64\n",
      " 20  dining_ic              4912 non-null   float64\n",
      " 21  presidential           4912 non-null   float64\n",
      " 22  client_segment         4947 non-null   float64\n",
      " 23  sect_empl              4947 non-null   float64\n",
      " 24  prev_stay              4947 non-null   float64\n",
      " 25  prev_all_in_stay       4947 non-null   float64\n",
      " 26  divorce                4947 non-null   float64\n",
      " 27  fam_adult_size         4947 non-null   float64\n",
      " 28  children_no            4947 non-null   float64\n",
      " 29  tenure_mts             4608 non-null   float64\n",
      " 30  tenure_yrs             4608 non-null   float64\n",
      " 31  company_ic             4947 non-null   float64\n",
      " 32  claims_no              4947 non-null   float64\n",
      " 33  claims_am              4973 non-null   float64\n",
      " 34  nights_booked          4947 non-null   float64\n",
      " 35  gender                 4947 non-null   object \n",
      " 36  shop_am                4947 non-null   float64\n",
      " 37  shop_use               4912 non-null   float64\n",
      " 38  retired                4947 non-null   float64\n",
      " 39  gold_status            4947 non-null   float64\n",
      " 40  score1_pos             1225 non-null   float64\n",
      " 41  score1_neg             1314 non-null   float64\n",
      " 42  score2_pos             1209 non-null   float64\n",
      " 43  score2_neg             1304 non-null   float64\n",
      " 44  score3_pos             1261 non-null   float64\n",
      " 45  score3_neg             1367 non-null   float64\n",
      " 46  score4_pos             1223 non-null   float64\n",
      " 47  score4_neg             1324 non-null   float64\n",
      " 48  score5_pos             1232 non-null   float64\n",
      " 49  score5_neg             1493 non-null   float64\n",
      " 50  outcome_profit         5000 non-null   float64\n",
      " 51  outcome_damage_inc     5000 non-null   int64  \n",
      " 52  outcome_damage_amount  5000 non-null   float64\n",
      "dtypes: bool(1), float64(50), int64(1), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Look at the descriptives\n",
    "1. For which features do you suspect outliers?\n",
    "2. Which of these outliers seem most suspicious? Which would you certainly check if you were able to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_am</th>\n",
       "      <th>profit_last_am</th>\n",
       "      <th>profit_am</th>\n",
       "      <th>damage_am</th>\n",
       "      <th>damage_inc</th>\n",
       "      <th>crd_lim_rec</th>\n",
       "      <th>credit_use_ic</th>\n",
       "      <th>gluten_ic</th>\n",
       "      <th>lactose_ic</th>\n",
       "      <th>insurance_ic</th>\n",
       "      <th>spa_ic</th>\n",
       "      <th>empl_ic</th>\n",
       "      <th>cab_requests</th>\n",
       "      <th>married_cd</th>\n",
       "      <th>bar_no</th>\n",
       "      <th>sport_ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3201.0</td>\n",
       "      <td>888.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4091.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income_am  profit_last_am  profit_am  damage_am  damage_inc  crd_lim_rec  \\\n",
       "0      227.0             0.0     3201.0      888.0         6.0      15000.0   \n",
       "1      268.0            16.0     1682.0        0.0         0.0        750.0   \n",
       "2      283.0            23.0     1673.0        0.0         0.0        750.0   \n",
       "3      227.0             0.0     1685.0        0.0         0.0          0.0   \n",
       "4     4091.0          1028.0     3425.0      785.0         2.0      14000.0   \n",
       "\n",
       "   credit_use_ic  gluten_ic  lactose_ic  insurance_ic  spa_ic  empl_ic  \\\n",
       "0            0.0        0.0         0.0           0.0     1.0      0.0   \n",
       "1            0.0        0.0         0.0           1.0     1.0      0.0   \n",
       "2            0.0        0.0         0.0           1.0     0.0      0.0   \n",
       "3            0.0        0.0         0.0           0.0     0.0      0.0   \n",
       "4            0.0        0.0         1.0           0.0     1.0      0.0   \n",
       "\n",
       "   cab_requests  married_cd  bar_no  sport_ic  \n",
       "0           3.0        True     2.0       1.0  \n",
       "1           7.0        True     3.0       0.0  \n",
       "2           1.0        True     4.0       0.0  \n",
       "3           6.0        True     8.0       1.0  \n",
       "4           4.0       False     2.0       1.0  "
      ]
     },
     "execution_count": 1233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:,0:16].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deze variabele is een boolean maar moet een getal worden\n",
    "\n",
    "train['married_cd'] = train['married_cd'].astype('int')\n",
    "train.loc[:, 'married_cd']\n",
    "\n",
    "score['married_cd'] = score['married_cd'].astype('int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_am</th>\n",
       "      <th>profit_last_am</th>\n",
       "      <th>profit_am</th>\n",
       "      <th>damage_am</th>\n",
       "      <th>damage_inc</th>\n",
       "      <th>crd_lim_rec</th>\n",
       "      <th>credit_use_ic</th>\n",
       "      <th>gluten_ic</th>\n",
       "      <th>lactose_ic</th>\n",
       "      <th>insurance_ic</th>\n",
       "      <th>...</th>\n",
       "      <th>score2_neg</th>\n",
       "      <th>score3_pos</th>\n",
       "      <th>score3_neg</th>\n",
       "      <th>score4_pos</th>\n",
       "      <th>score4_neg</th>\n",
       "      <th>score5_pos</th>\n",
       "      <th>score5_neg</th>\n",
       "      <th>outcome_profit</th>\n",
       "      <th>outcome_damage_inc</th>\n",
       "      <th>outcome_damage_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3201.0</td>\n",
       "      <td>888.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.838147</td>\n",
       "      <td>0.082288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1791.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.955259</td>\n",
       "      <td>1672.78</td>\n",
       "      <td>1</td>\n",
       "      <td>829.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>1.743020</td>\n",
       "      <td>1001.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.889793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1785.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4091.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330503</td>\n",
       "      <td>0.766294</td>\n",
       "      <td>0.490486</td>\n",
       "      <td>0.542445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3140.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   income_am  profit_last_am  profit_am  damage_am  damage_inc  crd_lim_rec  \\\n",
       "0      227.0             0.0     3201.0      888.0         6.0      15000.0   \n",
       "1      268.0            16.0     1682.0        0.0         0.0        750.0   \n",
       "2      283.0            23.0     1673.0        0.0         0.0        750.0   \n",
       "3      227.0             0.0     1685.0        0.0         0.0          0.0   \n",
       "4     4091.0          1028.0     3425.0      785.0         2.0      14000.0   \n",
       "\n",
       "   credit_use_ic  gluten_ic  lactose_ic  insurance_ic  ...  score2_neg  \\\n",
       "0            0.0        0.0         0.0           0.0  ...         NaN   \n",
       "1            0.0        0.0         0.0           1.0  ...         NaN   \n",
       "2            0.0        0.0         0.0           1.0  ...    0.099529   \n",
       "3            0.0        0.0         0.0           0.0  ...         NaN   \n",
       "4            0.0        0.0         1.0           0.0  ...         NaN   \n",
       "\n",
       "   score3_pos  score3_neg  score4_pos  score4_neg  score5_pos  score5_neg  \\\n",
       "0         NaN         NaN    0.838147    0.082288         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN    7.955259   \n",
       "2         NaN         NaN         NaN         NaN    0.101955    1.743020   \n",
       "3         NaN    0.889793         NaN         NaN         NaN         NaN   \n",
       "4    0.330503    0.766294    0.490486    0.542445         NaN         NaN   \n",
       "\n",
       "   outcome_profit  outcome_damage_inc  outcome_damage_amount  \n",
       "0         1791.66                   0                   0.00  \n",
       "1         1672.78                   1                 829.66  \n",
       "2         1001.40                   0                   0.00  \n",
       "3         1785.59                   0                   0.00  \n",
       "4         3140.74                   0                   0.00  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 1235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop_duplicates()\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replace all NaN values with '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN values with a specified value (e.g., 0)\n",
    "train.fillna(-1, inplace=True)\n",
    "train.head()\n",
    "\n",
    "score.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "na = train.isna()\n",
    "columns_with_na = train.columns[na.any()].tolist() \n",
    "print(len(columns_with_na))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Drop irrelvant columns\n",
    "\n",
    "gluten_ic and lactose_ic: The fact that a person is gluten or lactose intolerant does not indicate how likely it is for them to inflict damages or how much money they will be spending in the hotel. Maybe they will pay a tiny bit more for food without those ingredients but it shouldn't have a significant impact.\n",
    "\n",
    "cab_requests: The hotel is very unlikely to own the cab company so wether or not they buy a lot of taxis will not influence the profit for the hotel.\n",
    "\n",
    "marketing_permit: The choice on wether or not the marketing team may contact them will not influence how much money they will be spending nor how likely they are to inflict damages.\n",
    "\n",
    "region: Although region could be a small factor due to cultural diferences in spending and personal traits, this could lead to discrimination of people of a certain region.\n",
    "\n",
    "gender: Here it could also be that a cerain gender is for example more aggressive than others and thus more likely to inflict damages, but this could also lead to discrimination based on generalisations.\n",
    "\n",
    "divorce: Being divorced or not does not impact the way you behave or spend money, definitely not if some time has passed. Maybe the first months they could be a bit more aggressive or impulsive due to their grief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "train = train.drop(['gluten_ic', 'lactose_ic', 'marketing_permit','divorce', 'cab_requests', 'urban_ic', 'gender'], axis=1) #outcome_damage_inc, outcome_damage_amount\n",
    "score = score.drop(['gluten_ic', 'lactose_ic', 'marketing_permit', 'divorce', 'cab_requests', 'urban_ic', 'gender'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I make sure all three categorical features are classified as 'object' to be able to check if they are categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove unwanted outliers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No unwanted outliers found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the different ML models to predict projected revenue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Lineair Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dit algoritme heeft een score van 0.2842767456180185\n",
    "# # Split the dataset into training and testing sets\n",
    "\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['outcome_profit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Train a linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# testScore = model.score(X_test, y_test)\n",
    "# print('R^2 score on testing set:', testScore)\n",
    "\n",
    "# # Predict the projected revenue for the 500 applicants\n",
    "\n",
    "# score = scaler.transform(score)\n",
    "# predictions = model.predict(score)\n",
    "\n",
    "# # Sort the predictions in descending order\n",
    "# sorted_index = np.argsort(predictions)[::-1]\n",
    "# sorted_predictions = predictions[sorted_index]\n",
    "\n",
    "# print(sorted_predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Heeft een score van 0.025000182087153267\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# #TODO\n",
    "# # Split the dataset into training and testing sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['outcome_profit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Train a linear regression model\n",
    "# model = KNeighborsRegressor(n_neighbors=5)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model using r-squared score\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print('R-squared score:', r2)\n",
    "\n",
    "# # Predict the projected revenue for the 500 applicants\n",
    "\n",
    "# score = scaler.transform(score)\n",
    "# predictions = model.predict(score)\n",
    "\n",
    "# # Sort the predictions in descending order\n",
    "# sorted_index = np.argsort(predictions)[::-1]\n",
    "# sorted_predictions = predictions[sorted_index]\n",
    "\n",
    "# print(sorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dit algoritme heeft een score van 0.5646249872330892\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['outcome_profit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Create a decision tree regressor object\n",
    "# dt_regressor = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# # Fit the regressor with the training data\n",
    "# dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the revenue on the testing data\n",
    "# y_pred_dt = dt_regressor.predict(X_test)\n",
    "\n",
    "# # Compute R^2 score on the testing data\n",
    "# r2_score_dt = dt_regressor.score(X_test, y_test)\n",
    "# print(\"R^2 Score (Decision Tree Regression): \", r2_score_dt)\n",
    "\n",
    "\n",
    "# # Predict the projected revenue for the 500 applicants\n",
    "\n",
    "# score = scaler.transform(score)\n",
    "# predictions = model.predict(score)\n",
    "\n",
    "# # Sort the predictions in descending order\n",
    "# sorted_index = np.argsort(predictions)[::-1]\n",
    "# sorted_predictions = predictions[sorted_index]\n",
    "\n",
    "# print(sorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.4 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score on test set: 0.7581997911409797\n",
      "[17846.23930946 11656.47511537  9757.39631447  8993.66221965\n",
      "  8632.62538971  6937.95027024  6701.6313718   6626.91248729\n",
      "  5696.83395194  5422.13312826  5096.13705375  4388.45170801\n",
      "  4104.83206359  4058.5580197   4033.22972892  3988.19228134\n",
      "  3961.12130323  3840.81679253  3682.67951193  3644.02737093\n",
      "  3434.3115353   3305.26270993  3292.67570399  3224.77584496\n",
      "  3209.85267486  3183.03755976  3182.03116039  3178.56116212\n",
      "  3178.01261751  3150.32380139  3118.86096875  3111.97489783\n",
      "  3108.27960084  3097.86462032  3080.58879213  3037.7089059\n",
      "  2992.00036057  2944.68168403  2939.71958793  2935.0078479\n",
      "  2908.19224837  2905.5782214   2903.78753045  2899.95832351\n",
      "  2894.62213765  2886.67047926  2867.65556991  2860.76156046\n",
      "  2842.19121397  2828.70591461  2821.90510002  2791.38637017\n",
      "  2774.21360632  2762.7970799   2755.08146563  2742.47843079\n",
      "  2727.09734796  2692.54001034  2670.37019244  2638.06078216\n",
      "  2597.32120846  2590.50660105  2586.78539373  2555.41865904\n",
      "  2546.02824355  2505.28256738  2489.64626856  2475.10373847\n",
      "  2475.10373847  2462.54903741  2442.08765266  2438.21445111\n",
      "  2438.02853914  2420.79882611  2411.40594875  2376.67032859\n",
      "  2374.83445847  2362.90877601  2356.25886162  2340.92753933\n",
      "  2340.4210121   2324.1323851   2320.69622394  2304.12585222\n",
      "  2303.86508831  2295.00724243  2286.62466903  2261.72537139\n",
      "  2244.680218    2242.39196586  2233.55444368  2231.88327714\n",
      "  2229.88451433  2229.73755181  2221.27513693  2216.61878557\n",
      "  2213.21444525  2209.99193356  2202.67560116  2199.90545542\n",
      "  2198.79914057  2198.76435763  2184.43146444  2184.13385937\n",
      "  2179.52345411  2176.41356731  2170.41490476  2154.07963294\n",
      "  2150.106377    2148.3196357   2147.78025285  2145.09059422\n",
      "  2128.06320994  2121.65695931  2118.94106877  2116.84224368\n",
      "  2108.9004867   2108.3568057   2105.91609041  2103.05830115\n",
      "  2093.90055313  2093.8808995   2088.53226701  2086.78134649\n",
      "  2084.41367261  2080.60615216  2077.2092767   2077.2092767\n",
      "  2073.65760519  2073.29885954  2070.73348829  2068.27005039\n",
      "  2065.01804619  2062.47790135  2062.1813693   2062.14574292\n",
      "  2037.76050923  2031.20525927  2029.65480321  2028.93175796\n",
      "  2026.91320965  2020.43135769  2019.60367129  2012.50099439\n",
      "  2011.96217863  2008.97651404  2008.30135099  2003.25703455\n",
      "  1997.33046233  1995.97519937  1990.16011798  1988.64431974\n",
      "  1986.31296587  1978.10405423  1977.46620961  1973.98656451\n",
      "  1973.70540456  1973.30512565  1970.32940246  1969.73969128\n",
      "  1969.62727033  1966.13877602  1964.29128192  1963.82063061\n",
      "  1963.08604595  1961.92514975  1960.9575765   1952.23353869\n",
      "  1951.01742669  1948.7005887   1948.26717155  1941.59558884\n",
      "  1938.98749335  1937.70714659  1936.96195246  1936.46801531\n",
      "  1930.46412208  1928.05306618  1924.23022136  1922.50153239\n",
      "  1922.13284929  1921.52781855  1920.91261369  1920.19067172\n",
      "  1914.95541521  1914.31521883  1911.00430167  1908.61100131\n",
      "  1904.46751183  1902.81639796  1902.38413146  1900.70686753\n",
      "  1900.42524007  1896.12660035  1894.64344332  1894.48080671\n",
      "  1894.39318918  1888.43412317  1883.17136021  1880.04534597\n",
      "  1879.7396036   1879.44800593  1872.61909102  1864.45333537\n",
      "  1863.96168147  1861.5070417   1860.31042749  1857.33344261\n",
      "  1857.05393735  1855.32163894  1854.72235618  1850.54232696\n",
      "  1842.07044718  1841.93111977  1840.88306584  1840.70819507\n",
      "  1840.5696613   1838.77121962  1833.75258432  1829.44958177\n",
      "  1826.99849877  1825.84391238  1825.83725502  1822.62092334\n",
      "  1822.52441174  1821.49066255  1817.32295589  1816.7738528\n",
      "  1815.29321485  1812.6473683   1811.97848459  1811.30124378\n",
      "  1810.5144088   1809.62515586  1809.40888821  1809.1963187\n",
      "  1807.47109907  1804.56437656  1803.38323512  1802.14017327\n",
      "  1801.68462583  1801.11445953  1799.90106875  1799.42015386\n",
      "  1793.86246772  1793.32504468  1790.57756302  1789.55265167\n",
      "  1789.32112245  1788.89639286  1786.5114583   1784.52554377\n",
      "  1783.70249074  1782.51815101  1780.51629662  1776.7462026\n",
      "  1775.28926841  1774.46350798  1771.47034992  1771.21678425\n",
      "  1771.16780267  1768.74407013  1766.45742124  1766.21308937\n",
      "  1762.36805967  1761.02816335  1759.69672436  1757.59683423\n",
      "  1757.5401493   1755.39463227  1750.05194514  1747.95260116\n",
      "  1745.82869938  1745.62886366  1745.62886366  1744.82155128\n",
      "  1744.00749125  1741.99775878  1739.64372427  1738.60044243\n",
      "  1737.55715718  1737.09476087  1735.3640476   1728.72338335\n",
      "  1726.20059709  1725.22517484  1718.63561317  1715.20138889\n",
      "  1714.479332    1703.46558531  1703.03027369  1697.59535631\n",
      "  1696.39821406  1694.82784947  1692.46909228  1689.52234427\n",
      "  1680.93654379  1680.32119899  1677.88040606  1674.06948187\n",
      "  1671.28849708  1670.22887863  1664.09937508  1662.62146602\n",
      "  1657.45287262  1656.20359463  1653.40654728  1653.19632217\n",
      "  1650.29400797  1643.67037571  1643.3502979   1640.76454913\n",
      "  1637.04155995  1637.0276419   1635.7850245   1634.30820494\n",
      "  1632.705315    1631.13576197  1629.17375787  1628.01846241\n",
      "  1627.81862669  1625.41943494  1621.14461617  1619.05146236\n",
      "  1618.96372927  1613.1203436   1611.19598545  1601.7127625\n",
      "  1600.35090767  1599.50057731  1597.30758119  1596.6826783\n",
      "  1591.37111538  1590.47876721  1589.71284835  1586.24232513\n",
      "  1581.65473468  1580.82975386  1578.93096486  1577.06659408\n",
      "  1576.44494957  1570.08017164  1564.39330862  1563.31287859\n",
      "  1561.3926751   1547.70602446  1547.70161356  1544.30329197\n",
      "  1542.35207789  1538.27092016  1537.91016464  1537.29471705\n",
      "  1536.37889343  1535.17285383  1534.5198648   1534.29343261\n",
      "  1525.56651186  1525.54248134  1522.81902551  1518.43463722\n",
      "  1517.48143407  1517.24586158  1512.48276657  1512.48224323\n",
      "  1505.05058435  1501.37098864  1501.2525141   1499.98225686\n",
      "  1498.69248675  1494.4886277   1493.84626352  1493.25232738\n",
      "  1492.03759536  1490.84031462  1490.34929166  1490.27819012\n",
      "  1481.84868339  1481.77046576  1480.98289428  1479.29194713\n",
      "  1478.74338229  1476.64805275  1475.92354032  1475.92354032\n",
      "  1473.81574054  1465.62067949  1464.88756836  1463.15155801\n",
      "  1460.96393719  1460.29467805  1459.73349439  1455.52775772\n",
      "  1452.74778217  1452.57754773  1452.34618675  1450.18173439\n",
      "  1449.23128342  1440.93530294  1437.79802386  1437.24258499\n",
      "  1436.9873612   1436.70020548  1435.01365183  1434.93390541\n",
      "  1432.95413614  1430.0819597   1429.72184228  1427.691706\n",
      "  1425.75155507  1422.11487376  1418.78774217  1413.59633258\n",
      "  1413.079241    1412.71358264  1412.24110312  1408.9542567\n",
      "  1408.67492848  1408.00644067  1407.74143144  1405.44656295\n",
      "  1404.94714494  1401.61139288  1398.3506876   1397.24839382\n",
      "  1395.51337105  1394.66667008  1388.99321293  1387.64429722\n",
      "  1386.94249287  1384.44466797  1384.24651155  1379.79338999\n",
      "  1379.75025094  1378.28792224  1374.84278294  1373.68342005\n",
      "  1364.04478779  1361.3862221   1358.3069333   1352.74229754\n",
      "  1350.5401107   1349.01647705  1344.03806083  1341.11926704\n",
      "  1340.40336352  1334.86090626  1333.7585099   1328.91800178\n",
      "  1322.78124071  1321.25691796  1320.2518951   1319.67143013\n",
      "  1316.75580265  1315.2451333   1305.97056753  1305.8516044\n",
      "  1305.67691407  1297.96643308  1296.54187987  1294.71206519\n",
      "  1291.68441274  1286.62471185  1285.80003505  1283.84038732\n",
      "  1267.34190474  1266.46013801  1261.01357904  1255.47685895\n",
      "  1253.57328361  1247.21730316  1246.44299149  1246.04717412\n",
      "  1238.78820239  1232.51727244  1226.25972087  1219.44711146\n",
      "  1219.35173103  1219.04107829  1218.65796064  1218.41316682\n",
      "  1218.05876463  1212.19987932  1211.55655184  1205.07015676\n",
      "  1202.61079806  1186.47927771  1181.93508976  1180.2185146\n",
      "  1178.27673172  1177.07669289  1176.18368906  1175.44724592\n",
      "  1165.94742326  1148.90931078  1116.7244299   1111.80435865\n",
      "  1078.82161286  1069.69807578  1017.5221781    994.92403387]\n"
     ]
    }
   ],
   "source": [
    "# # Dit algoritme heeft een score van 0.7581997911409797\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['outcome_profit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Instantiate the model\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test data\n",
    "# y_pred = rf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model using r2 score\n",
    "# from sklearn.metrics import r2_score\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"r2 score on test set:\", r2)\n",
    "\n",
    "# # Predict the projected revenue for the 500 applicants\n",
    "\n",
    "# score = scaler.transform(score)\n",
    "# predictions = model.predict(score)\n",
    "\n",
    "# # Sort the predictions in descending order\n",
    "# sorted_index = np.argsort(predictions)[::-1]\n",
    "# sorted_predictions = predictions[sorted_index]\n",
    "\n",
    "# print(sorted_predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.5 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3362.4398629  3027.54848201 3019.50948947 3010.44316598 2996.49824914\n",
      " 2989.66793367 2969.66374457 2947.36871681 2940.6829894  2940.6829894\n",
      " 2936.23512247 2932.64399686 2932.64399686 2930.77918847 2919.90775708\n",
      " 2918.04294869 2918.04294869 2918.04294869 2918.04294869 2913.67387244\n",
      " 2910.66312417 2910.66312417 2910.66312417 2897.92688439 2897.47140235\n",
      " 2897.47140235 2897.26771638 2897.26771638 2897.26771638 2892.89864013\n",
      " 2884.73516257 2884.73516257 2884.73516257 2884.73516257 2884.73516257\n",
      " 2884.73516257 2884.73516257 2884.73516257 2870.36560773 2869.50662669\n",
      " 2867.62984655 2867.62984655 2867.62984655 2865.13310028 2863.95993026\n",
      " 2863.95993026 2863.95993026 2863.95993026 2860.21774385 2851.84147501\n",
      " 2848.73139437 2848.37663549 2838.89690385 2834.73615898 2831.97047943\n",
      " 2820.46829327 2813.96092666 2807.73205349 2807.73205349 2790.62673746\n",
      " 2786.95682117 2786.95682117 2786.95682117 2782.43333018 2782.11106806\n",
      " 2782.11106806 2782.11106806 2780.64838647 2779.80364563 2769.85150515\n",
      " 2760.4416034  2760.4416034  2759.02841331 2753.06177888 2752.40261087\n",
      " 2742.76783812 2741.92309728 2740.3255391  2736.59205858 2730.23528432\n",
      " 2726.56536803 2726.56536803 2726.56536803 2723.8558188  2723.8558188\n",
      " 2723.8558188  2719.55030679 2703.08058649 2691.20053276 2687.8520506\n",
      " 2657.86087595 2646.85270972 2646.85270972 2638.19326585 2629.74739369\n",
      " 2603.42279814 2589.76072492 2586.46125658 2581.27280009 2564.73863527\n",
      " 2548.81910975 2544.38351273 2544.38351273 2544.38351273 2544.38351273\n",
      " 2544.38351273 2531.48451739 2523.60828042 2522.16675492 2506.50296439\n",
      " 2504.19292971 2494.40730747 2486.20689188 2473.63207516 2463.35399883\n",
      " 2459.68408254 2457.51802012 2457.51802012 2457.51802012 2457.51802012\n",
      " 2457.51802012 2456.52675913 2454.44592685 2443.28608941 2443.27574179\n",
      " 2440.41270409 2438.45948065 2438.45948065 2438.45948065 2438.30834138\n",
      " 2438.30834138 2438.24345676 2436.7427878  2436.7427878  2432.02326382\n",
      " 2432.02326382 2432.02326382 2426.45593509 2426.45593509 2421.13814073\n",
      " 2419.63747178 2419.2498019  2415.81058006 2412.01376108 2411.9968893\n",
      " 2405.68070278 2405.68070278 2404.23917728 2404.23917728 2404.23917728\n",
      " 2404.23917728 2403.5359204  2403.10342438 2402.14448588 2402.14448588\n",
      " 2401.5701933  2401.5701933  2401.5701933  2398.19078163 2398.19078163\n",
      " 2395.24924073 2395.03534774 2394.14271548 2393.14814897 2393.14814897\n",
      " 2393.14814897 2393.14814897 2393.14814897 2393.14814897 2393.14814897\n",
      " 2393.14814897 2393.14814897 2393.14814897 2393.14814897 2393.14814897\n",
      " 2393.14814897 2393.14814897 2393.14814897 2393.14814897 2393.14814897\n",
      " 2393.14814897 2393.14814897 2393.14814897 2393.14814897 2393.14814897\n",
      " 2393.14814897 2393.14814897 2393.14814897 2390.951959   2389.56664774\n",
      " 2389.56664774 2389.56664774 2389.56664774 2389.56664774 2383.46394496\n",
      " 2382.36051455 2382.36051455 2382.36051455 2381.64254142 2379.04677064\n",
      " 2377.93003171 2377.54570834 2377.41554931 2376.04283295 2376.04283295\n",
      " 2376.04283295 2376.04283295 2376.04283295 2375.97402381 2372.82618436\n",
      " 2372.46133171 2372.37291666 2372.37291666 2372.37291666 2372.37291666\n",
      " 2372.37291666 2372.37291666 2372.37291666 2372.37291666 2372.37291666\n",
      " 2372.37291666 2372.37291666 2372.37291666 2372.37291666 2372.37291666\n",
      " 2372.37291666 2369.79563826 2368.79141542 2368.79141542 2367.65339268\n",
      " 2367.65339268 2367.65339268 2367.34988992 2367.34988992 2367.34988992\n",
      " 2366.35862893 2365.25519852 2364.53722539 2363.02881681 2361.4316533\n",
      " 2361.30149428 2361.30149428 2361.30149428 2361.30149428 2361.30149428\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2356.25886162 2356.25886162\n",
      " 2356.25886162 2356.25886162 2356.25886162 2355.86559894 2355.26760063\n",
      " 2355.26760063 2355.1987915  2352.43527542 2352.05095204 2351.6860994\n",
      " 2350.54807665 2349.02040594 2348.26133448 2347.45594421 2346.87816036\n",
      " 2346.87816036 2346.57465761 2344.32633727 2344.19617825 2342.2535845\n",
      " 2342.06280831 2341.95836912 2340.65642099 2340.52626196 2339.15354559\n",
      " 2339.15354559 2339.15354559 2339.15354559 2339.15354559 2339.15354559\n",
      " 2339.15354559 2339.08473646 2335.72878067 2335.4836293  2335.4836293\n",
      " 2335.4836293  2335.4836293  2335.4836293  2335.4836293  2335.4836293\n",
      " 2335.4836293  2335.4836293  2335.4836293  2335.4836293  2335.4836293\n",
      " 2335.4836293  2331.15601845 2330.76410532 2330.76410532 2330.76410532\n",
      " 2330.76410532 2330.76410532 2330.76410532 2330.76410532 2330.76410532\n",
      " 2330.76410532 2330.76410532 2330.76410532 2330.76410532 2330.76410532\n",
      " 2330.76410532 2330.76410532 2330.76410532 2330.76410532 2329.77284433\n",
      " 2329.46934158 2323.55110496 2323.55110496 2323.55110496 2321.97942043\n",
      " 2321.97942043 2318.62346464 2318.37831328 2318.37831328 2318.37831328\n",
      " 2318.37831328 2318.37831328 2318.37831328 2318.30950414 2316.25480354\n",
      " 2316.25480354 2316.25480354 2314.95354836 2314.95354836 2310.49118619\n",
      " 2310.23402437 2309.98887301 2309.98887301 2307.99595073 2307.99595073\n",
      " 2307.99595073 2307.99595073 2307.99595073 2299.14948752 2299.14948752\n",
      " 2292.88355698 2292.88355698 2289.45879206 2289.45879206 2287.22071841\n",
      " 2282.94701742 2282.94701742 2282.94701742 2282.94701742 2282.94701742\n",
      " 2282.94701742 2282.94701742 2282.94701742 2282.50119443 2282.50119443\n",
      " 2282.50119443 2282.50119443 2271.10666337 2271.10666337 2271.10666337\n",
      " 2271.10666337 2271.10666337 2271.10666337 2271.10666337 2271.10666337\n",
      " 2271.10666337 2271.10666337 2271.10666337 2271.10666337 2265.8417014\n",
      " 2262.17178511 2262.17178511 2262.17178511 2262.17178511 2261.72596211\n",
      " 2254.00134735 2254.00134735 2254.00134735 2250.33143106 2250.33143106\n",
      " 2250.33143106 2250.33143106 2250.33143106 2250.33143106 2245.61190708\n",
      " 2245.61190708 2245.61190708 2245.61190708 2245.06646908 2245.06646908\n",
      " 2245.06646908 2236.67702881 2236.35834784 2233.22611503 2228.50659105\n",
      " 2224.83667476 2214.14159003 2212.11306155 2210.59162377 2204.35181691\n",
      " 2193.48630775 2182.1350591  2146.16351694 2146.16351694 2142.96543073\n",
      " 2127.39653156 2119.92931218 2114.15698601 2114.15698601 2114.15698601\n",
      " 2114.15698601 2114.15698601 2104.18436672 2103.56344462 2093.38175369\n",
      " 2093.38175369 2076.27643767 2076.27643767 2076.27643767 2076.27643767\n",
      " 2076.27643767 2074.26518027 2062.93447722 2056.73033911 2050.93099106\n",
      " 1976.03500173 1968.82605211 1927.19525492 1872.46412222 1839.0165281\n",
      " 1797.88951703 1797.35259817 1774.40550376 1764.8233973  1764.72521643\n",
      " 1763.35726921 1760.14928363 1748.1331348  1737.56904879 1730.39394449\n",
      " 1721.3665787  1710.57550597 1709.84405026 1705.47177872 1695.55314144\n",
      " 1695.22210406 1683.48603036 1673.19991427 1655.27833332 1655.27833332\n",
      " 1648.51953912 1629.23813903 1612.03600535 1611.70083693 1576.31929217\n",
      " 1552.07444841 1532.35563144 1454.24104572 1447.25791072 1342.81427994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joery\\.virtualenvs\\project_venv-7qiVcfMQ\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Dit algoritme heeft een score van 0.771681378659433\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['outcome_profit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = GradientBoostingRegressor()\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate the R-squared score on the testing data\n",
    "# # r2 = r2_score(y_test, y_pred)\n",
    "# # print(\"R-squared score on testing data:\", r2)\n",
    "\n",
    "# # Predict the projected revenue for the 500 applicants\n",
    "\n",
    "# score = scaler.transform(score)\n",
    "# predictions = model.predict(score)\n",
    "\n",
    "# # Sort the predictions in descending order\n",
    "# sorted_index = np.argsort(predictions)[::-1]\n",
    "# sorted_predictions = predictions[sorted_index]\n",
    "\n",
    "# print(sorted_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Damages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the 500 applicants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_inc']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit decision tree regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "#     scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit decision tree regressor to entire training set\n",
    "# dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained decision tree regressor\n",
    "# damages_pred = dec_tree.predict(new_applicants_stand)\n",
    "# print(damages_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_inc']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit decision tree regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "#     scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit decision tree regressor to entire training set\n",
    "# dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained decision tree regressor\n",
    "# damages_pred = dec_tree.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_inc']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit KNN regressor with cross-validation\n",
    "# k_values = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for k in k_values:\n",
    "#     knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#     scores = cross_val_score(knn, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit KNN regressor to entire training set\n",
    "# knn = KNeighborsRegressor(n_neighbors=5)\n",
    "# knn.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained KNN regressor\n",
    "# damages_pred = knn.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.5 Lineair Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_inc']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit linear regression model\n",
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained linear regression model\n",
    "# damages_pred = lin_reg.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_inc']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit random forest regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     rnd_forest = RandomForestRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "#     scores = cross_val_score(rnd_forest, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit random forest regressor to entire training set\n",
    "# rnd_forest.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained random forest regressor\n",
    "# damages_pred = rnd_forest.predict(new_applicants_stand)\n",
    "# print(damages_pred) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Predict damage amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO\n",
    " # Use all ML algorithms to predict damages:\n",
    "    # - Linear Regression x\n",
    "    # - Decision Tree x\n",
    "    # - KNN x\n",
    "    # - Random Forest x\n",
    "    # - Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit Gradient Boosting regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     gb_regressor = GradientBoostingRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "#     scores = cross_val_score(gb_regressor, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit Gradient Boosting regressor to entire training set\n",
    "# gb_regressor.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained Gradient Boosting regressor\n",
    "# damages_pred = gb_regressor.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit decision tree regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     dec_tree = DecisionTreeRegressor(random_state=0, max_depth=d)\n",
    "#     scores = cross_val_score(dec_tree, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit decision tree regressor to entire training set\n",
    "# dec_tree.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained decision tree regressor\n",
    "# damages_pred = dec_tree.predict(new_applicants_stand)\n",
    "# print(damages_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit KNN regressor with cross-validation\n",
    "# k_values = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for k in k_values:\n",
    "#     knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#     scores = cross_val_score(knn, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit KNN regressor to entire training set\n",
    "# knn = KNeighborsRegressor(n_neighbors=5)\n",
    "# knn.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained KNN regressor\n",
    "# damages_pred = knn.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import Random\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # split into train and test sets\n",
    "# X = train\n",
    "# X = X.drop(['outcome_damage_amount', 'outcome_damage_inc', 'outcome_profit'], axis=1)\n",
    "# y = train['damage_am']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # standardize numerical features\n",
    "# num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "# num_feat = [feat for feat in num_feat if feat in X_train.columns and feat in score.columns] # remove non-existent features\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train[num_feat])\n",
    "# X_train_stand = X_train.copy()\n",
    "# X_test_stand = X_test.copy()\n",
    "# X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "# X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "# # fit random forest regressor with cross-validation\n",
    "# depth = np.arange(1, 50)\n",
    "# cv_scores = []\n",
    "# sd_scores = []\n",
    "# for d in depth:\n",
    "#     rnd_forest = RandomForestRegressor(random_state=0, n_estimators=100, max_depth=d)\n",
    "#     scores = cross_val_score(rnd_forest, X_train_stand, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    "\n",
    "# # fit random forest regressor to entire training set\n",
    "# rnd_forest.fit(X_train_stand, y_train)\n",
    "\n",
    "# # Standardize numerical features for new applicants\n",
    "# num_feat = [feat for feat in num_feat if feat in score.columns] # remove non-existent features\n",
    "# new_applicants_stand = score.copy()\n",
    "# new_applicants_stand[num_feat] = scaler.transform(score[num_feat])\n",
    "\n",
    "# # Predict damages for new applicants using the trained random forest regressor\n",
    "# damages_pred = rnd_forest.predict(new_applicants_stand)\n",
    "# print(damages_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# To select the 200 applicants we will subtract the predicted damages from the predicted revenue\n",
    "# Test the ML algortihms to determine which one is the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
